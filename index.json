
[{"content":"","date":"2025 年 5 月 17 日","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":" 终于！搬运工作结束了！之前的随笔记录基本上都从hexo迁过来了！\n吐槽一下：之前的数学公式有点不太对劲，要全部换掉格式了 虽然说只要在md的页面上加这个就行, REF\n原来inline都是$ ... $的格式，现在是\\\\( ... \\\\)的格式了 本周简单的总结一下 因为好像差不多一个月没有更新过了www # 主要总结一下论文画图相关的总结还有经验（仅供参考，不全对ooo\n总结一下(糊)论文的方法 # 写在前面 当时的场景是5/1小长假里面速通论文 5/7日晚上就要交了，所以质量必然高不了的\n同时，因为是网络领域的会议，写作方法肯定和主流的AI会议不太一样，所以仅供参考 :)\nAI会的Paper be like: # 拉上一堆作者 **+(无贬义)\n对话框 + 图形\n炫酷的emoji + LLM各种图标 + 色彩轰炸(存疑)\n超绝大表格 各种sota横向对比\n消融实验：为什么我们的贼jb厉害，薄纱现有的框架\n具体原理是基本木有的/数学公式写出来不是让人看懂的(?) 👆xhs上面偷的一张图 很好的概括了\n回到网络会议 # 要点1 # Abstract准确精炼，5句话最多了\n要点2 # Intro前面快速切入主题，最后点出本文贡献\n要点3 # 模块设计图要够大 够清晰； 考虑配色和图形对比（色相对比 or 直曲对比 or 形状对比） 要点4 # 论文的结构要清晰，尤其是对比实验的部分\n实验图清晰易懂\nFindings # conclusion随便写因为不重要，约等于复述摘要\nreference部分的格式\n问题1: 怎么画论文图呢 # 工具使用 # 省流；最简单的方法就是有请Claude 3.7/ChatGPT帮你画图\n人工方法：直接用drawio画 / PPT其实也不错 （但是记得都要导出为pdf格式 且dpi像素要至少300以上）\n图片数量 # 背景部分1张 + (一般是会涉及到系统的实现)1张overview图 (关键) + 系统里面的各个小模块图(n个) + 实验图(m个) + 实验表格\n实验图的部分和前面的模块图分开来解释，因为实验图基本上就是和测量的数据强相关的，只需要能够表达出 系统的关键指标的变化 和 对比其他实现的提升幅度 / 特殊情况下为什么会下降 就可以的。\nmatplotlib全家桶 + 基本的折线 / 柱状图 / 散点图 / subplots\n复合图形在同一张图上面（常见的是折线+柱状 / 折线+散点这样的）\n背景图 1张 # 关键性：⭐⭐⭐⭐\n论文背景除要在Introduction里面提到的内容，考虑到其实很多reviewer可能根本不懂你在做神魔，要尽量简单易懂就会偷懒先看图，而不会在意你在文章写了什么。\n因此背景图（background部分）就需要做一件事情：\n一个足够简单和清晰，但是有代表性的case 例子：我们的研究的具体应用是怎样的 （可以有1句话的图解在图片边上） Overview图 1张 # 关键性：⭐⭐⭐⭐⭐\n内容需要完整的表达出系统的设计思路和实现的关键点（就是你 做了什么）\nFRAMEWORK TITLE 超大的好吧\nSUB BLOCKS 加粗\n多用箭头 状态转移 / 流程 非常清晰，能够看到处理顺序 / 数据的流向\n粒度划分：\n系统构成分块（模块化）（大）：宏观认知，边界清晰，够抽象 各个模块的功能（中） ：对于具体细节的概括，精炼 关键功能的实现细节 （小） 具体例子：你的算法 / 用了什么工程设计技巧 / 可以细一点，但是不要每个都细 这样我第一眼就能够明白 做了大概是什么东西，涉及了哪些模块，亮点会在系统什么模块的什么部分\n👆 这个例子里，我能够看到的是阶段分割 （训练 + 推理） 然后训练里面 好像是参考 经过了什么编码 + 风格的色块 / 内容色块（大概是风格知识 / 内容知识） 经过的是UNET（虽然我不知道内部会发生什么）\n然后参数上面能够看出编码器是冻结的（固定），但是红色的风格构成模块是可变的（对比色） 最后我得到了目标的什么东西\n那么我就知道了 这篇文章的大概贡献都在Q-Former这个模块里面\n推理也是类似的，但是感觉是为了完整性才放上来的。 （上面的分析全是我自己瞎猜的，我根本不会AI orz）\n小模块图 n张（取决于你想要展开哪些部分） # 关键性：⭐⭐⭐\nfocus on 细节 和 值得关注的tricks / 技巧 / 算法思想点\n(可能是一个反面例子)\n这是我随手画的一个反面例子，对于大部分没有接触过领域知识的人来说，可能就只能get到的信息量有：\n输入被变成了一个vector\n还有一部分的vector从什么地方抽取过来\n经过了一个aggergate的操作\n经过了一个Regressor\n输出了一个结果 \\(X \\% \\) 但是其实内部的一些技巧都没有展示出来：\nvector的具体含义是什么（内部构成是什么） additional的操作是什么 （怎么进行的向量合成呢？） expected node上的数据怎么被抽取的（实际上需要额外的metric系统，daemons） 为什么需要GBR而不是采用别的模型呢？ GBR内部的设计是什么呢？ 这些其实都是缺失的，所以就会给人一种看不懂的感觉；\n反而是一些Prediction / Total这些组件其实可以省略\ngraph LR; A[向量是什么]--\u003eA1[系统的什么指标]; A[向量是什么]--\u003eA2[指标含义和原理]; A1[系统的什么指标]--\u003eB[组合原理]; A2[指标含义和原理]--\u003eB[组合原理]; B[组合原理]--\u003eB1[组合方式]; B[组合原理]--\u003eB2[组合有效性]; B1[组合方式]--\u003eC[GBR的执行流程]; B2[组合有效性]--\u003eC[GBR的执行流程]; C[GBR的执行流程]--\u003eD[跳转到系统下游]; 👆 重新拆解了一下\n实验图 n张 # 关键性：⭐⭐\n我相信只需要实验的结果足够多，这个图完全没有难度吧（纯数据驱动xs）\n核心：你的方案是在太强了！！！ 薄纱所有其他人（就是这种感觉）就ok 🌶\n","date":"2025 年 5 月 17 日","externalUrl":null,"permalink":"/posts/2025/week20/","section":"Posts","summary":"","title":"Week20总结","type":"posts"},{"content":"I\u0026rsquo;m really happy you stopped by.\n","date":"2025 年 5 月 17 日","externalUrl":null,"permalink":"/","section":"Welcome page","summary":"","title":"Welcome page","type":"page"},{"content":" Kubernetes 网络感知调度的研究进展 # 随着云原生应用复杂度提升，多微服务应用或 NFV 服务链对网络性能敏感。传统 Kubernetes 调度器只关注 CPU/内存，很少考虑网络延迟和带宽。近年来，社区与研究界提出了多种网络感知调度方案：如正在发展的 Scheduler Plugins 项目下的 “Network-Aware Scheduling” KEP，目标在 Pod 调度时纳入网络拓扑和带宽因素 scheduler-plugins.sigs.k8s.io\n例如，Santos 等提出的 Diktyo 框架通过自定义 AppGroup 和 NetworkTopology CRD，结合 TopologicalSort（拓扑排序）和 NetworkOverhead 插件来减少微服务间的网络开销 scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 。此外，Kubernetes 1.23 开始支持拓扑感知路由（Topology Aware Routing），在服务发现时优先选择同可用区的 Endpoints，以降低跨区通信延迟 kubernetes.io 。K8s 原生的 Pod 亲和/反亲和也可指定节点、可用区等拓扑键，实现应用对网络拓扑的基本约束 kubernetes.io kubernetes.io 。 Koordinator (阿里云开源) 则在 QoS 调度层面引入网络控制：1.5 版合作支持 Terway CNI 的网络 QoS，允许为 Pod 或 QoS 类别设置带宽上限，并支持动态调整，使得在多业务共存时可限制 Pod 带宽防止相互干扰 koordinator.sh 。Volcano（批处理调度器）最新版本已内置“网络拓扑感知调度”功能，在分布式训练等场景下考虑节点间通信拓扑来降低跨节点开销 volcano.sh 。此外，Kubernetes 生态中还有针对 NUMA 拓扑（CPU/内存亲和）的插件（如 LeastNUMANodes Scoring），但它们主要关注节点内资源亲和，与网络感知调度侧重点不同。 网络信息感知与获取 要实现网络感知调度，首先需收集底层网络拓扑和性能指标。常见方法包括：一方面通过主动探测和监控采集网络延迟与带宽，例如 Diktyo 中的 netperf 组件会定期测试可用节点间的时延，并将结果写入 ConfigMap scheduler-plugins.sigs.k8s.io 。NetworkTopology 控制器读取这些测量值，计算不同区域/可用区之间的网络代价和可用带宽，并更新自定义的 NetworkTopology CRD scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 。为了降低全节点对测量开销，该方案假设同一区内延迟相近，仅对跨可用区链路进行网速测试 scheduler-plugins.sigs.k8s.io 。另一方面，CNI 插件也可提供网络性能数据。例如 Kubernetes 社区提供的带宽 CNI 插件（如 Calico 内置的 bandwidth plugin）支持对 Pod 网络流量进行入口/出口限速，通过在 Pod 注解中设置 kubernetes.io/ingress-bandwidth 和 egress-bandwidth 来强制带宽上限 scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 。同时，调度器可将节点物理网络带宽声明为扩展资源（例如 network.aware.com/bandwidth），并据此进行调度算分 scheduler-plugins.sigs.k8s.io 。此外，在 SDN 或云数据中心环境下，也可通过网络控制器（如 OpenFlow/OVS 查询）或链路探测协议获取拓扑信息；高级 CNI（如 Cilium/Hubble）提供流量和延迟监控，可为调度决策提供实时网络指标（如延迟、丢包率、吞吐量等）。 网络相关调度策略与算法 在调度策略层面，研究和实践结合了多种网络因素： 延迟感知调度：优先考虑微服务间通信延迟。在多层应用或 NFV 服务链中，将强依赖的服务副本调度到网络距离较近的节点上。例如 Diktyo 中的 TopologicalSort 插件依据服务拓扑（AppGroup 中定义的依赖顺序）排序 Pod，配合 NetworkOverhead 插件在筛选/打分时偏向于网络开销小的节点 scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 。这种做法旨在减少链路延迟，参考了服务功能链 (Service Function Chaining) 的思想 scheduler-plugins.sigs.k8s.io 。 带宽感知调度：将网络带宽作为受限资源纳入调度。调度器在节点过滤和评分时，优先满足带宽需求高的 Pod。具体而言，可将节点的可用网络带宽作为扩展资源（extended resource）在调度请求中声明，例如 Diktyo 框架将带宽容量以 network.aware.com/bandwidth 的形式暴露 scheduler-plugins.sigs.k8s.io 。这样，像 PodFitsHostResources 或 BalancedAllocation 这样的算分策略都能使用带宽维度进行评估。配合 CNI 带宽限速插件，还可实现对 Pod 带宽配额的管理，避免网络拥塞 scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 。 网络亲和/反亲和：利用 Kubernetes 原生亲和性规则根据网络拓扑分组 Pod。用户可指定 podAffinity 或 podAntiAffinity，设定“如果某 Pod 在 X 拥有标签 Y，则本 Pod 应（或不应）也部署在同一拓扑域 X 内”，其中 X 可是同一节点、同一可用区、同一机架等 kubernetes.io 。最新的拓扑感知路由功能则在服务层面优先选择同可用区内的 Endpoint，尽量让流量“留在原区域”以降低跨区通信开销 kubernetes.io 。 服务链调度：对于网络功能虚拟化 (NFV) 中的服务链或多微服务事务，需要考虑端到端路径。调度时可将组成链的微服务视为一组，根据它们之间的调用关系进行联合调度。例如，在 Diktyo 框架中，通过 AppGroup CRD 定义微服务拓扑，TopologicalSort 插件会按照该服务链顺序下发 Pod scheduler-plugins.sigs.k8s.io ；确保链上节点间网络开销最小，从而满足业务的时延要求 scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 。类似地，Network Service Mesh 等方案也着重在多节点网络上创建可编排的服务链路。 NFV与边缘场景的网络感知调度 在边缘计算和 NFV 场景下，网络感知调度需求更为突出：边缘节点网络条件多变（带宽低、延迟高），NFV 服务链对 QoS 要求严格。近期研究针对这些场景提出了专门方案。例如，邱颖等人针对边缘节点网络差异大、服务链通信密集的特点，提出了基于 TOPSIS 的 NACS 算法。该算法动态获取集群节点的实时网络与资源信息，按低时延、低丢包目标进行综合评估，将链路延迟和带宽纳入调度考量 link.springer.com 。实验证明，与默认调度相比，NACS 可显著改善网络性能，在实际工作负载下 Web 服务和 Redis 吞吐量分别提升约26%和29.6% link.springer.com 。另一个例子是 Tsokov 等提出的 Kinitos 框架，专注于移动集群环境（如车辆、无人机组网）。它在 Kubernetes 调度器基础上增加了动态 反调度（迁移）机制，可在节点位置变化或网络变差时，将依赖微服务迁移到更优网络位置，保障链路性能 colab.ws 。此外，在 5G MEC（多接入边缘计算）和跨区域云边协同部署中，调度器需考虑更大范围的拓扑（如跨地区节点通信），并结合网络监测数据选择就近计算资源，以满足低时延和可靠性需求。 面临的挑战与优化方法 将网络纳入调度决策带来了诸多挑战：网络状态的时效性和可观测性较差，网络性能（延迟、带宽）会随流量、链路拥塞或节点移动快速变化，使得实时监测与反馈难度大。主动探测大规模集群中的全对节点网络参数开销极高，因此必须简化测量策略（如只对关键路径或跨区链路采样） scheduler-plugins.sigs.k8s.io 。调度算法变得多目标更复杂，需要在 CPU/内存资源和网络指标之间权衡；如果只优化网络，可能导致计算资源利用率下降。因此，多目标优化或加权评分常被采用，在满足网络需求的同时兼顾资源利用。 另外，现有 Kubernetes 特性有时与网络感知调度相冲突。例如默认的 Service 负载均衡会打破稳定的网络路径：Pod 间流量被随机分发，多跳网络环境下难以计算确定的带宽需求。Moeyersons 等人指出，为保证端到端带宽需要，必须禁用自动负载均衡，将流量静态路由到固定节点对 imec-publications.be 。同时，他们总结了在无线网格网络上使用 K8s 的问题：如多跳网络中获取吞吐量指标非常困难 imec-publications.be 、环境干扰使网络极易变化 imec-publications.be 等。 为了克服这些问题，已有工作采取了多种优化措施。典型做法是将网络度量采集与调度解耦：例如上述网格网络方案引入了独立组件——**网格拓扑控制器（mesh topology controller）配合各节点上的 agent 定期测量延迟和带宽，并更新专用的拓扑 CRD imec-publications.be ；同时使用网格连接控制器（mesh connection controller）**在网络中建立或维护静态路由，将数据流限制在预定路径 imec-publications.be 。这种设计可以使调度器仅关注经处理后的网络信息，减轻自身负担。另外，利用 CNI 插件的 QoS 能力也是关键手段：如使用 Calico 带宽限速确保各 Pod 带宽可控 scheduler-plugins.sigs.k8s.io 。Kubernetes 原生的拓扑路由特性也可在网络层面提供改进，通过优先同区流量降低时延 kubernetes.io 。综合来说，目前的研究主要通过多组件协同（度量采集器 + 调度插件 + 网络控制器）和算法优化，尽可能在动态网络下为调度器提供稳定的网络视图，降低调度决策和网络实现间的矛盾。 参考文献： 已引用文章和项目均为近年成果，包括 KEP 文档 scheduler-plugins.sigs.k8s.io scheduler-plugins.sigs.k8s.io 、Koordinator 博客 koordinator.sh 、Volcano 官网 volcano.sh 、相关研究论文 link.springer.com imec-publications.be 等。以上资料从学术论文、开源实现到官方文档，全面反映了 2023–2025 年期间网络感知调度的最新进展。\n","date":"2025 年 5 月 4 日","externalUrl":null,"permalink":"/posts/2025/my-second-post/","section":"Posts","summary":"","title":"Kubernetes 网络感知调度的研究","type":"posts"},{"content":"记录一下Leetcode分数的变化\n现在感觉瓶颈不在题量上，而是思维。。。 cf就很麻烦，一直都是在1600分左右徘徊，感觉思维还是不够灵活，要加油了（这是AI写的，我根本不知道自己在说什么，但是感觉就是很真实）\nCopilot：你很棒棒哦！（这是AI自己补充的\n","date":"2024 年 11 月 26 日","externalUrl":null,"permalink":"/posts/2024/2024-11/","section":"Posts","summary":"","title":"2024年唯一一篇记录 好奇怪呢","type":"posts"},{"content":"","date":"2023 年 10 月 7 日","externalUrl":null,"permalink":"/tags/5g/","section":"Tags","summary":"","title":"5G","type":"tags"},{"content":"","date":"2023 年 10 月 7 日","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"蜂窝网络控制程序（例如，移动性、空闲-活动转换以节省能源）直接影响数据平面行为，影响用户体验延迟。 L25GC实现了分页，并实现了避免3GPP的智能切换方案。\n摘要 # 蜂窝网络控制程序（例如，移动性、空闲-活动转换以节省能源）直接影响数据平面行为，影响用户体验延迟。 识别此控制数据平面相互依赖，L25GC重新构建5G核心（5GC）网络及其处理，以减少控制平面操作的延迟和它们对数据平面的影响。 利用共享内存，L25GC消除消息序列化和 HTTP 处理开销，同时符合 3GPP 标准。我们改进数据平面通过分解函数进行处理，以避免控制数据平面干扰和使用可扩展的流级数据包分类器转发规则查找。 利用5GC的缓冲区，L25GC实现了分页，并实现了避免3GPP的智能切换方案。\n发夹路由，以及 5G 基站有限缓冲导致的数据丢失，减少延迟和不必要的消息处理。 L25GC 的集成故障恢复能力可透明地从5GC软件网络功能和硬件故障很多 比 3GPP 的重新连接恢复程序更快。L25GC 建成\n基于 free5GC，一个基于开源内核的 5GC 实现。L25GC 将多个控制平面事件的事件完成时间缩短了 ∼50%，并改善了数据包延迟（由于改进了控制平面通信） 在寻呼和切换期间 ∼2×事件，与free5GC相比。L25GC的设计是通用的，尽管当前实现支持有限数量的用户会话。\n背景介绍 # 在5G蜂窝生态系统中，实现了蜂窝组件； 作为基于软件的云原生服务，实现部署灵活性。但是，随着5G核心网（5GC）中的控制平面过程与传统的4G类似[49]，它没有充分利用全部优势。\n网络功能（NF）的软件化。 蜂窝核心预计将经历更多的控制平面流量，由于：\n蜂窝用户的大量增长，包括物联网和机器对机器设备[31] 频繁切换：由于单元格大小减小而导致的事件。用户事件完成时间，比如一个需要1.9秒的交接过程（如[45]所示），会直接影响最终用户数据包。 除了由于传统而受到的处罚蜂窝控制平面核心程序，我们识别几个额外的会导致延迟的问题（详见 §2.3）： 采用HTTP / REST进行NF间通信，表面上是为了支持“解聚合”的想法，但存在开销。消息序列化和 TCP 处理。 由 3GPP 标准驱动的当前实施（§5.2.1[8]），在列表中组织转发（匹配操作）规则并执行查找匹配规则的低效线性搜索。 用户在切换期间可能会遇到更高的数据包丢失到当前的发夹和菊花链路由，以及有限的缓冲区在目标 gNB（5G 基站）上。 在故障期间，当前的3GPP恢复过程要求用户重新启动连接建立请求并重新开始。这直接影响正在进行的数据连接（例如，TCP 体验）虚假超时），影响 QoE。 什么是hairpin和daisy-chain路由\n相关工作 # 降低控制平面的延迟：Neutrino做到快速序列化+大的地理范围内维护用户状态的副本 优化5GC的SBI：Buyakar 通过gRPC取代HTTP/REST的方式实现服务接口 蜂窝核心网络的可用性提升：ECHO 提出了4G 分布式 状态机复制的协议 重新设计控制平面流程：CleanG 可扩展的基于NFV的架构优化控制/数据平面的延迟。 共享内存(DPDK特性) 利用客户段的状态： DPCM 客户端的解决方案-初始化+执行控制操作 与此同时 利用用户设备端的状态信息来降低控制平面的延迟 现有框架的挑战 # 挑战1：控制消息的处理 # 实际上，相对于4G，5G中交换的控制消息数量略有增加，以保持用户状态在多个分解NF中的一致性[6]。 交换这些消息的成本很高，由于几个因素，包括消息序列化、HTTP/TCP 开销。 通信开销将更高：如果 NF 放置在节点上。这成为一个问题，因为5G网络部署和应用不断发展，例如支持物联网，这可能会增加控制平面流量[31]。\n挑战2：复杂的交接流程 # 切换操作最多可能需要 1.9秒 [45] 完成。这可能会影响数据平面流量。 例如，基于 TCP 的数据流量可能会遭受虚假超时，这会降低应用程序吞吐量。\n与4G基站不同，gNB可能相对较小，缓冲能力有限。\n每个基站的缓存大小约为1300个完整的MTU(每一个radio resource-connected的UE)\nX2/S1等数据转发技术带来额外的延迟+数据丢失的问题\nhairpin-back 流量返回5G核心网络 5GC, 再转发到目标的gNB\n挑战3：开销很大的规则匹配过程 # UE将不仅仅是指用户的一个具体的移动端设备 一个5G的家庭网关可以同时操作多个5G通信的会话，请求不同类型的服务 数据包将会携带不同层级的QoS信息\n5GC需要提供一个可变的QoS模型，QoS的粒度需要设计在每个数据流上。 引出了 多IP层级的网络数据流模型=5G网络场景，因此就需要\n{% asset_img user_session.png 一个用户的会话中所涉及的多个数据流 %}\n目前试图在5G的数据平面中提供的过滤字段\nService Data Flow (SDF) filter Ethernet Packet Filter 在数据包中添加过滤字段 = 大量的PDR(Packet Detection Rules)被按照每个数据包的频率进行扫描\n挑战4：网络功能的可靠性和可恢复性 # 从故障中回复，3GPP提出了需要额外的控制信息，用于恢复5G网络Network Function中的内容\n等待完成的连接需要等待一系列的上下文完成恢复之后才能够进行。但是这一过程的延迟巨大，而且可能带来数据包丢失的问题。\nThis need for reattachment can be avoided by replicating the primary NFs. 就是说可以通过复制最初的NF来实现避免恢复上下文的操作。\n在主数据库发生故障时，主动复制需要状态信息切换到活动备用数据库，但是，为此类 NF 维护强一致性副本可能会影响正常性能。\n尽管现有技术（例如，使用延迟检查点和完全检查进行数据包重放指向）在NFV环境中运行良好，5GC构成显着影响\n由于控制平面和数据平面之间紧密的相互依赖性，需要对其设计进行实质性的增强，因此存在挑战。\n此外，持续运行的副本会消耗系统资源。我们需要一个策略来缩短恢复时间，协调控制平面状态与数据平面，并确保一致性无需消耗额外资源。\n3. L25GC的设计和实现 # 优化处理和通信，减少控制和数据平面的延迟，实现高吞吐量\n实现 3GPP 指定的控制面板和数据平面\n3GPP 5GP 面向服务架构的基本思想是 5G 规范面向服务架构背后的基本思想是，允许独立开发和扩展单个服务。 的基本思想是允许按照微服务设计范式独立开发和扩展单个服务。\n3GPP 5G 规范面向服务架构背后的基本思想是允许按照微服务设计范例独立开发和扩展单个服务。\n不过，我们认为，基于服务的接口不应强制要求其必须只使用 HTTP 和 REST API。\n在微服务共同驻留在微服务上的情况下，应能利用其他方式交换信息。 但在微服务共同驻留在同一节点上时，应能利用其他方式交换信息。 在同一节点上整合NF # 我们利用系统功能，包括用于数据共享的共享内存空间，以避免在 NF 之间移动数据。 共享内存空间，避免在 NF 之间移动数据，这也避免了序列化和去序列化的额外成本。\n序列化和去序列化的额外成本。这也使我们能够减少网络 I/O 延迟，并有助于克服目前推荐的 SBI 所产生的开销。\n具体实现：\n共享内存的通信机制：支持5G核心网络中不同网络功能(NF)之间的快速通信\nNF Manager单元： handle incoming packets, manage the shared memory, and facilitate zero copy communication between 5GC NFs\n5GC-NF拥有一个独一的服务ID和Rx/Tx传输队列 这些环形缓冲区与用于数据包 指向共享内存中数据包的数据包描述符的管理器共享。 处理后 NF 会将元数据附加到数据包上，以指定该数据包的操作（发送到端口/NF、丢弃 (发送到端口/NF、丢弃），并将其放入发送环，供管理器处理。管理器处理该操作。\n要在NF 之间发送数据包时，源 NF 会在元数据中指定目标 NF 的 ID。 管理器会将扁平数据结构的描述符复制到目标 NF 的环形缓冲区中，从而减轻序列化和HTTP 处理。 我们用基于描述符的共享内存方法取代 SBI 和 N4 接口（图 1 所示）替换为基于描述符的共享内存方法，从而消除了大量开销。 我们利用 OpenNetVM 的共享内存 [60] 所提供的简洁抽象来实现 数据的序列化和无锁进程间通信。\n实现了generic cGO shim layer\nShim是什么? 每一个 Containerd 或 Docker 容器都有一个相应的 \u0026ldquo;shim\u0026rdquo; 守护进程，这个守护进程会提供一个 API，Containerd 使用该 API 来管理容器基本的生命周期（启动/停止），在容器中执行新的进程、调整 TTY 的大小以及与特定平台相关的其他操作。shim 还有一个作用是向 Containerd 报告容器的退出状态，在容器退出状态被 Containerd 收集之前，shim 会一直存在。\nshim 将 Containerd 进程从容器的生命周期中分离出来，具体的做法是 runc 在创建和运行容器之后退出，并将 shim 作为容器的父进程，即使 Containerd 进程挂掉或者重启，也不会对容器造成任何影响。这样做的好处很明显，你可以高枕无忧地升级或者重启 Containerd\n基于Go实现的网络功能可以使用DPDK的功能 以及ONVM提供的共享内存特性。 Zero-Cost State Update # 划分数据平面 UPF-C \u0026amp; UPF-U\n转发规则和UPF状态都是存储在共享内存内的。\n使用共享的Huge Page + 两个hash table存储用户会话上下文位置的指针 TEID 和 UEIP 用于区分 UL 和 DL链路的流量。（上传链路和下载链路）\nPDRs and FARs：用来控制数据包转发行为\nSecurity Domain # L25GC 中的安全域：云环境中可能运行着多种服务（可能由第三方开发）\n在云环境中运行，可能会出现窃听和数据篡改等安全问题。\n有必要将 L25GC 与其他应用程序隔离。\n我们利用了 [42] 的安全域设计。 L25GC 的信任模型假定 L25GC 的所有 NF 都是可信的、 这些 NF 共享一个私人内存池。 这些 NF 共享同一个节点上运行的其他应用程序无法访问的私有内存池。 节点上运行的其他应用程序无法访问。在 L25GC 启动时，作为 DPDKprimary 运行的 NF 管理器（图 3）会自动启动。 为 L25GC 中的 NF 创建一个私有共享存储器池。这个私有共享内存池在创建过程中指定了独特的 \u0026ldquo;共享数据文件前缀\u0026rdquo;[13]。\nL25GC 中的 NF 作为 DPDK 二级进程运行，使用 NF 管理器指定的相同文件前缀 NF 管理器指定的相同文件前缀来访问私有共享内存池。\n对于同一节点上由不同操作员管理的多个 L25GC 实例，每个实例都有一个唯一的文件前缀。操作员管理的多个 L25GC 实例，每个实例都有一个唯一的文件前缀，以保持其共享内存池与其他实例隔离。\n进一步增强L25GC 安全域设计的进一步增强将是增加一个准入控制机制，以验证寻求访问属于蜂窝运营商的专用内存池的网络节点。\n智能切换缓冲 # hairpin: hairpin中文翻译为发卡。bridge不允许包从收到包的端口发出，比如bridge从一个端口收到一个广播报文后，会将其广播到所有其他端口。bridge的某个端口打开hairpin mode后允许从这个端口收到的包仍然从这个端口发出。\n直接实施 3GPP 移交规范的直接实施涉及在源 gNB 缓冲下行链路数据包。在源 gNB 上缓冲下行链路数据包，而源 gNB 的资源可能有限，特别是在小基站中。\n为了解决这个问题，我们利用 UPF 已经为寻呼操作提供的缓冲功能，而不增加任何额外的控制信息。这样做的另一个好处是避免通过源 gNB 进行发夹式路由。\nL25GC 不引入任何额外信息来启用缓冲。不过，它修改了控制平面（SMF）行为，以便在 UPF 为切换事件提供缓冲规则。 当 UE 请求切换时，SMF 将发送数据包转发控制协议 (PFCP) 消息。SMF 向 UPF 发送数据包转发控制协议 (PFCP) 消息，为目标 gNB 分配新的 TEID。 我们利用这一机会，在原始 PFCP报文的基础上附加一个 IE，利用PFCP 消息更新 PDR 和 FARaction，以缓冲数据包等。 快速规则匹配 # 由于 UPF-U 需要对每个到达的数据包进行 PDR 查询，以确定其转发策略，因此查询速度直接影响到数据平面的性能。 我们研究了不同的替代方案：线性搜索 (PDR-LL)、TSS（PDR-TSS）和分区排序 (PDR-PS)以降低大规模 PDR 查询的搜索复杂度。\n我们认为这对 5GC 的未来发展至关重要。\n线性查找 - TSS 和 PartitionSort\nPDR-TSS 和 PDR-PS 的搜索复杂度都比 PDR-LL 低。\nPDR-TSS 根据元组（如 PDI IE 字段）将 PDR 划分为多个子表，从而降低了搜索复杂度。同一子表中的 PDR 在每个元组中具有相同的前缀位、 每个子表都组织为一个具有 O（1） 复杂度的哈希表，用于基于 TSS 遍历元组空间的 PDR 查找（即，一组从 PDR 转换的子表），直到找到匹配的 PDR。 与基于链表的 PDR-LL 相比，当存在大量 PDR 时，PDR-TSS 搜索可实现更少的开销。包含 M 元素的链表可以是转换为 N 元组 （N ≤ M）。 但是，PDR-TSS 不能保证最佳的查找性能，因为分区子表的数量有一些变化。 在最坏的情况下，PDR-TSS可能具有与PDR-LL相同的搜索复杂性。PDR-TSS 搜索开销的可变性，加上需要软件哈希来查找子表，可能会导致高开销。\nPDR-PS： PDR-PS通过利用多维二叉搜索来降低搜索复杂性。 与链表的顺序查找相比，PDR-PS 可以在排序的 PDR 中执行快速二进制搜索。\n类似于PDR-TSS，PDR-PS 将 PDR 分成多组，然后对这些组进行排序以进一步降低查找的复杂性。\n为了适应打包的 5GC，我们采用了许多 PDI IE（最多到 20） 以支持所需的丰富功能，包括防火墙、NAT和每流 QoS 处理（请参阅附录A 代表更多细节）。\n通过状态复制实现弹性 # external synchrony 外部的同步机制 避免了服务故障导致的性能缺陷\n系统实现：基于free5GC以及OpenNetVM（一个高性能的NFV平台，基于DPDK技术）\n后面就是详细解释作者如何实现以上的优化的。\n两种粒度的可恢复性\nLocal resiliency\nlocal replica of each NF 网络功能副本 通过 cgroup的freezer进行冻结 直到NF Manager唤醒这个NF再启动副本 no-replay scheme 的方式进行同步 保证输出的commit只有一个 NF不会将响应发送回客户端 直到本地的副本完成复制（由于处于同一个系统上 同步时间很短） 如果出现故障：Manager直接拉起副本 保证了一致性 Remote resiliency：当服务节点不可用\n引入外部的同步过程\nLB节点管理计数器 在每一条发送出去的消息上绑定一个计数器，在缓存区内维护消息的副本 packet logger划分到4个不同的队列上去 UL-control, UL-data, DL-control, DL-data 缓存的数据包被重新复制一次状态（在副本程序内） replica node挑选4个队列中counter之最小的作为重现数据包的开始，从而实现按照顺序进行状态的复制。\n主节点的本地备份：周期性发送状态的快照给远程的副本 本地和远程都维护一个record用来处理和同步消息 每一个UE的事件触发一次同步 从而可以在故障时完成数据包的恢复，防止重传； 5GC需要处理大量的控制平面事件，需要大量的event-based checkpointing\nFailure detector：探测网络服务是否出现了故障，Seamless-BFD\n","date":"2023 年 10 月 7 日","externalUrl":null,"permalink":"/posts/2023/low-latency-5g/","section":"Posts","summary":"","title":"SIGCOMM' L25GC论文研究","type":"posts"},{"content":"","date":"2023 年 10 月 7 日","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2023 年 10 月 7 日","externalUrl":null,"permalink":"/categories/%E7%A7%91%E7%A0%94/","section":"Categories","summary":"","title":"科研","type":"categories"},{"content":"","date":"2023 年 10 月 7 日","externalUrl":null,"permalink":"/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/","section":"Tags","summary":"","title":"云原生","type":"tags"},{"content":"雀(PS 现在已经雀圣2了)\n水一水麻将\n打了好久力\n日常5个搭子数不清楚，比如334/556这种好型+对子，有的时候需要保留对子的对碰进张，有些时候需要拆对子又会不舍得.jpg\n还有就是什么时候应该进攻，什么时候应该防守的问题。\n平场，可能面对子家的立直，一向+手上1dora好型就可以蹭（立直dora1是2600点），但是面对亲的立直/染手对对副露，就好像不太能对攻起来（因为7700/11600的一半/三分之一大于了我们的期望收入）。因为这里标准定在1/2所以导致偏向保守的更多，所以能看到点炮率低的同时，和到的概率也低。\n07.25\n再水一下 目前100多把打到了雀豪1 1100/2800的菜鸡\n感觉很多时候对于局势的判断会比较困难，是否进攻还是直线全防的问题，比起牌效率的影响更大了一点呢。\n测试以下是否可以刷新一次contribution. 在设置了public权限之后\n10.07\n狗上豪3\n","date":"2023 年 5 月 28 日","externalUrl":null,"permalink":"/posts/2023/mahjong/","section":"Posts","summary":"","title":"雀魂の记录","type":"posts"},{"content":"","date":"2023 年 2 月 13 日","externalUrl":null,"permalink":"/tags/%E5%BF%AB%E5%8E%BB%E5%AD%A6%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"快去学算法","type":"tags"},{"content":" ABC 289F # 给你一个矩形区域, 用a,b,c,d 标识 矩形区域 $R = {(x,y) | a - 0.5 \\leq x \\leq b + 0.5,\\ c - 0.5 \\leq y \\leq d + 0.5}$ 定义为这个区域内的所有整点.\n初始, 你在$(s_x,s_y)$; 问你是否可以通过$n( 0\\leq n\\leq 10^6)$次以下操作达到点$(t_x,t_y)$?\n操作： 假设你当前在点$(s,t)$\n选取矩形区域内任意整点$(x,y)$, 做中心对称操作翻转到点$(x\u0026rsquo;,y\u0026rsquo;)$\n用公式表示就是: $x\u0026rsquo;=2x-s,\\ y\u0026rsquo;=2y-t$\n如果可以, 请打印操作过程; 不行则输出No\n思考 # 考虑一个点的翻转操作, 容易发现这是对称的, 两次操作抵消; 其次考虑两个相邻点$(x,y),(x+1,y)$, 假设绕着$(x,y)$翻到了$(2x-s,2y-t)$, 在从这个点借助$(x+1,y)$翻转一次, 就到了$(2x+2-2x+s,2y-2y+t) = (s+2,t)$这个点; 发现了上面这个规律之后, 就容易发现$x$和$y$可以分开操作, 上面演示了横坐标的变化规律,纵坐标同理; 翻转的上界? 这种操作方式最多不超过$2*2e5$, 肯定是符合要求的. 题解 # 0213 CF 1324E # https://codeforces.com/contest/1324/problem/E\n输入 n(≤2000) h L R (0≤L≤R\u0026lt;h≤2000) 和长为 n 的数组 a(1≤a[i]\u0026lt;h)。\n对于每个 a[i]，你可以把它减一，或者保持不变（换句话说，每个 a[i] 至多 -1 一次）。 定义前缀和 s[0]=a[0], s[i]=s[i-1]+a[i]。 如果 s[i]%h 落在闭区间 [L,R] 内，则分数加一。 最大化分数\n思路 # 先考虑搜索的写法: 假设我们当前选择第i个数字a, 之前的前缀和为s,已知\n可以选择$a[i]$, $(s\u0026rsquo; = s + a[i]) % h$ 可以选择$a[i] - 1, $(s\u0026rsquo; = s + (a[i] - 1)) % h$ 如果$L\\leq s\u0026rsquo; \\leq R$ 那么+1 否则等于 $max(dfs(i + 1, s+a[i]), dfs(i + 1, s+a[i]-1))$ 找到了子问题之后就容易解决了, 再来考虑边界情况\n初始 dfs(0, 0) 假设下标从0开始 边界: i = n, 所有都选完了, 当前状态直接返回0 题解 # ","date":"2023 年 2 月 13 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-0213/","section":"Posts","summary":"","title":"有趣的算法题-0213","type":"posts"},{"content":" 02.03 ARC 119C # 连续两个数字同时+1/-1\n需要反映到一些\u0026quot;不变量\u0026quot;上去 一个数组的奇数和 和 偶数和的相对大小是不变的! 交错和之差不变 统计子数组和为K的数量的算法\n前缀和+hashtable 是已知的 交错和相等的子数组就是答案 =\u0026gt; 奇数乘以+1,偶数乘以-1,得到的子数组和恰好为0 =\u0026gt;转化成子数组和为0的问题 =\u0026gt; 容易解决\n其他记录:\nABC 288D # 前缀类型套路+思维题\nABC 288E # 给定一组需要买的物品B1,B2,\u0026hellip;BK\n如何安排一个最好的顺序去买这K个物品，花费最少\n当你买$B_i$时, 如果之前你已经买了$B_1,B_2,\u0026hellip;,B_{i-1}$中的$x$件物品 那么第$B_i$实际上对应第$B_i - x$个没有被售出的商品\n买$B_i$花费$A_{B_i} + C_{B_i - x}$ 由于$x$属于$0$到$i - 1$ 所以花费至少是$A_{B_i} + cost(B_i, i-1)$\n$cost(i,j) = min{C_{i-j},\u0026hellip;, C_{i-2},C_{i-1}, C_{i} }$\n买所有的物品花费总和 至少需要 $\\sum_{i=1}^k (A_{B_i} + cost(B_i, i-1))$\n证明这个下界是可以取到的\n当你想要买物品$i$的时候, 取决于之前买了多少物品,记作$j$件 那么买的代价就是$A_i + cost(i,j)$\n所以可以列出这个状态\n$dp[i][j]$表示 前$i$个物品中, 买了$j$个物品的最小代价\n$dp[i+1][j+1] \u0026lt;- min{dp[i+1][j+1], dp[i][j]+cost(i+1,j) }$ $dp[i+1][j] \u0026lt;- min{dp[i+1][j], dp[i][j]}$\n通过预处理$cost(i,j)$我们可以通过一个$n^2$的dp解决问题\nABC 288F # 给你一个数字组成的字符串X,长度为N, 保证没有0\n考虑一个${1,2,3,..,N-1}$的一个子集$S$\n定义$f(S)$等于 断开$S$中对应位置后\n剩余数字字符串得到的乘积\n问这个和是多少?\n例如\n234\n2 | 34 2 | 3 | 4 23 | 4 234\n容易得到的一个dp方法: $dp_j = \\sum_{i=0}^{j-1} dp_i * X[i+1:j], \\forall i \u0026lt; j$\n然后可以发现喜提TLE(太慢了 N2)\n如何优化这个dp的转移? $dp_j = dp$\n注意到$X[i:j] = 10X[i:j-1] + X[j]$\n$$ dp_j = \\sum_{i=0}^{j-1} dp_i * X[i+1:j] = \\sum_{i=0}^{j-1} dp_i * (10X[i+1:j-1] + X[j:j]) = 10dp_{j-1} + X[j:j] \\sum_{i=0}^{j-1} dp_i $$\n可以发现变成了一个递推+前缀和的形式 于是就可以简化计算到$O(N)$\n","date":"2023 年 2 月 3 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-0130/","section":"Posts","summary":"","title":"灵茶の试炼-0130","type":"posts"},{"content":"","date":"2023 年 1 月 31 日","externalUrl":null,"permalink":"/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"我参考了一个优秀的mini-Go语言入门项目, 用来熟悉一下Go语言的基本语法\u0026amp;特点;代码总量不到500行, 复杂语法只有一个Getter.\n","date":"2023 年 1 月 31 日","externalUrl":null,"permalink":"/posts/2023/go-learn/","section":"Posts","summary":"\u003cp\u003e我参考了一个优秀的mini-Go语言入门项目, 用来熟悉一下Go语言的基本语法\u0026amp;特点;代码总量不到500行, 复杂语法只有一个Getter.\u003c/p\u003e","title":"Go学习01_实现分布式缓存","type":"posts"},{"content":"2023新年之前的最后一杯茶\n01.16 CF 1691D # 所有的子数组满足$max(a) \\geq sum(a)$, 输出YES, 否则输出NO;\n$O(N)$复杂度 看到子数组的max =\u0026gt; 单调栈, 每个元素作为max的范围可以求出来 =\u0026gt; 在这个范围内, 找到子数组的最大和(可以用前缀和+维护区间最值的数据结构完成) =\u0026gt; 优化: 子数组最大和的检查, 可以放在单调栈弹出的时候检查(检查中间一段的和是否大于0(为什么? 因为单调栈维护最大值 所有小于的值必然会在达到边界直接出栈, 我们要找的负数必然会先出栈, 所以直接在这个时候检查即可))\n01.17 CF 1409E # 两条长度为$k$的线段, 覆盖最多的端点, 问可以最多覆盖多少点?\n类似的题目：m条线段，LC 2209 01.18 ABC 222F # 换根dp\n求每个节点出发到其他节点的最大expense 端点带权重, 所以不仅需要考虑路径的最大值, 也要考虑只有一个端点(邻居节点+当前边), 端点为当前节点+最长路径等情况 01.19 CF 1721D # b数组可以任意排列\n$(a[1]^b[1]) \u0026amp; (a[2]^b[2]) \u0026amp; \u0026hellip;\u0026amp; (a[n] ^b[n])$的最大值 从高到低排列bit位, 尽可能取1\n问题：高位保证1的时候, 低位怎么搜索? 搜索+保持状态 | 使用数学\u0026amp;奇妙的掩码 01.20 CF 1598D # 策略：\n正难则反：直接数符合要求的数对很麻烦，因此要想到总数-不满足要求的数量 从n点取3点，横坐标互不相同/纵坐标互不相同\n注意题目的重要信息：任何两个坐标不可能完全相同； 因此如果3个点的x坐标都一样，y坐标不可能相同； 那么不符合条件的数对只有两个x坐标相同且两个y坐标相同的情况 ","date":"2023 年 1 月 20 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-0116/","section":"Posts","summary":"","title":"灵茶の试炼-0116","type":"posts"},{"content":" 01.09 CF 1370D # 二分+分类\n01.10 CF 1358D # 滑窗 =\u0026gt; 月份中的每一天是否都需要考虑? =\u0026gt; 优化滑窗\n证明：窗口的末尾在月末能够得到最优解（反证法）\n于是就可以用双指针快乐的模拟了 01.11 CF 547B # 长为x的连续子数组中最小值的最大值\n连续子数组+最小值 = 单调栈\n维护每个数组元素作为最小值的 范围长度, 则小于等于这个长度的子数组最小值可以是这个值\n对于每个长度获取最大值(后缀最大值), 反向遍历\n01.12 CF 1490G # 前缀和+二分 前缀和循环情况下的二分\n向下取整 01.13 CF 777D # 贪心+排序\n","date":"2023 年 1 月 20 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-0109/","section":"Posts","summary":"","title":"灵茶の试炼-0109","type":"posts"},{"content":"2023的第一杯茶\n01.02 CF 379D # 01.03 ABC 249F # 初始你有一个数字\\(x = 0\\), 你可以进行两种操作, 总计\\(N\\)个操作, 操作1: x替换为y 操作2: x等于 x加上y\n你总共可以跳过至多\\(K\\)个操作, 问你可以得到的最大x是多少?\n操作1会覆盖之前的所有操作记录, 从后往前考虑是一个合理的选择 从后往前, 首先记录操作1的数量op1, 与K进行比较; \\(op1 \\leq K\\)都至少可以跳(不被覆盖掉) 遇到操作2, 贪心地选最好的进行, 否则就跳掉(用一个堆来维护?), 所以肯定是保留正数, 负数尽可能挑大的. 01.04 # 01.05 CF 1718A2 # 输入t组数据, 每一组数据包括一个数字n和长度为n的数组\n每次操作可以把a的下标从L到R的元素同时异或一个数, 花费为\\(ceil(R - L + 1) / 2\\)\n输出把a的元素全部变成0的最小代价\n尽可能地连续 尽可能覆盖更多位?\n感觉可以和今天的每日一题放在一起, 都是处理数值异或类型的问题.\n01.06 CF 1739D # 你可以做如下操作至多 k 次： 断开 p[i] 和 i 之间的边，然后在 1 和 i 之间连边。 输出操作后，这颗树的最小高度。 高度的定义为 1 到最远叶子节点的路径的边数。\n最小高度 =\u0026gt; 可以二分这个高度值 什么时候我们需要做操作：自底向上写一个树形dp，当高度值+1达到限制时，需要进行切割操作； 记录这个次数与k比较（是否可行），不断二分求出最小高度 ","date":"2023 年 1 月 3 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-0102/","section":"Posts","summary":"","title":"灵茶の试炼-0102","type":"posts"},{"content":"水,摸鱼,touch 🐟\n2022年的总结:\n1月-2月感觉还是很普通的\n1月就稍微忙一下学校里的期末考试 :)感觉没有什么很印象深刻的事情发生了, 可能是因为2021年太正常了\n写了一个很简单的mail-server+mail-client, 试用了一下electron(前端兼职客户端), emm node这一套和web2确实有很大的不同感觉, 可以让我这个辣鸡后端直接全包了😂, 当然最终优点摸鱼, 有一些东西没有做的完全(比如特殊格式, 邮件附件之类的就咕了, 扔github上了)\n2月的时候就有一种感觉(上海不太正常),2月底刚开学的时候学校里就封了一次(从那个时候开始, 一有什么风吹草动就马上开润成为了一种常态), 感觉就很离谱(对比18年，19年的时候)，完全没有大学生活的感觉了(全是网课,而网课约等于0(还不如自学..)), 也不是很想吐槽这种管理方式(懒得细说了), 后来就是正常的找暑期实习(其实那个时候一个问题就是日常实习也完全ok, 事实证明毫无差别,只要能去就一定要去,绝对比留在学校里好多了,学术大佬看到这一段可以自动省略了)\n2月底开始投各种公司(大厂)的实习, 然后也拿到了好几个笔试(志杰,开水厂), 网易阿里还有很多我都忘了(迫真); 因为谁也不认识于是就开始乱搞一波, 印象比较深的是志杰笔试题(4题2小时), 在那个时候我觉得还是挺难的(挺有趣的)。逛各种论坛(牛客之类的)看别人的面经, 我自己也贡献了好几篇, 交流中发现好像今年的难度开始提上去了(没有以前这么简单), 不过我觉得也正常\u0026hellip; 本人是自己做了3.5/4的笔试题成绩足够混过去了(志杰的笔试), 而开水团的笔试就很离谱(我做的那一场就混了1.7/5分), 事实证明没有什么影响, 也不会有人关心你做的怎么样, 更多的都是实际问题(实际工程问题, 八股文我是很反感去死记硬背的所以我根本没去准备).\n于是就经常出现各种奇怪的脑洞\u0026amp;面试官的不满\u0026hellip;, 直到3月底都没有什么消息(草), 后来发现全挂了hhh, 可能就不是一路人的感觉, 所以面试官直接turn over了(悲); 有很多人都说刷题, 我个人的理解是已经把做算法题变成脑筋急转弯了, 单纯就是一种爱好, 那个时候我的目光还只局限于LC, 靠着xjb做题也搞了接近1000题?(其实你会发现根本不会有人真的从1000题这么大的范围里面抽, 而且未必你就一定会啊? 所以这个算法题我觉得就完全是锦上添花的东西, 你只需要有基本的能力, 能不能AC, 没有bug这些并不重要, 高级数据结构和背题为了面试而准备真的不划算, 不如多看看DDIA呢)\n3月到4月事情有一点点多, 首先就是SH沦陷了, 我还清楚的记得3/13号的时候, 虽然表面上风平浪静, 但是各种小道消息都在传这里封那里封的, 那个时候就赶紧去准备屯东西了(事实证明非常正确, 但是没有备够,建议去篱笆论坛逛一逛, 这种奇奇怪怪的东西往往能在各种本地论坛先看到\u0026hellip;)\n细说一下后来的实习经历我觉得真的是很有意思了, 当时正好入职的时候是风控的末期(最后几天了), 因为电脑断货的问题成功拿到了windows机器😓, 正好是端午节之后入职(有好多人都是那一天啊hhh); 之前mentor就已经来发消息了(那个时候还有点怀疑是不是假的消息xd), 人真的是很nice(对于我这样一个完全不用java技术栈), 6月的时候有好多的环境问题和一些基本的java环境问题都是找mt远程解决的(感觉就是给mt不停的提oncall了, 我现在感觉还是可以精简一下很多的内容和问题的, 虽然说起来都是小事但是真的会很占用别人的精力), 这个时候有好多问题也是去问在之前就入职的另一个实习生大哥(感谢, 当然后来他成功转正了,祝好); 6月就这么迷迷糊糊的过去了, 中途就接了一个小bug的修复,交了一次代码(其实是搭便车和别人的一起上了), 然后也体验了下产品(体验其实还不错但是就是功能很分散, 然后很多东西从字面上你是猜不出是做什么的, 甚至后来问team里的人发现只有开发者才知道是啥, 其他方向的可能一点都不知道(太尴尬了))\n7,8,9,10,11这5个月我感觉是大学里最充实的一段时间了, 去年22届的新员工也是在这个时候恢复线下的(return-offer的感觉);\n就说几个最重要的点：\n一个应用整合了很多内网的应用的时候, 各个方面都需要考虑，最靠谱的就是走数据库，数据库也做好备份，性能问题总是排在最后考虑的 并行程序总是会出各种问题，在串行可用的情况下，做对照，一定要保证你的并发结果和串行一致才可以逐渐迁移过去，人脑真的是串行的，并行程序一次写对真的几乎不可能（即使前人给你提供了很多的经验） 业务虽然有的时候挺无聊的，但是你总是能发现各种奇怪的实现思路以及奇怪的要求，用户提出的很多都是无效需求。。。 经验绝对比你的算法水平更重要，我接手参与的代码系统里最复杂的算法也就是tarjan求割点和拓扑排序，这比起算法题的难度可以说完全就是可忽略不计的。 虚拟环境，虚拟机器都是好东西，同时这种多点服务，多机的运维（没错，甚至是debug/运维你都能学到各种奇怪的知识），运维的过程中终于明白了devops这种奇怪的缩写到底在干啥，后来自己就复刻了一个\u0026hellip; 关键在于多用，用完了别忘，总是可能会再遇到的qwq 11月离职后到现在主要就是感觉自己又变得很摸鱼, 虽然完成了一个小目标 (LC上瓜了, 感谢灵の茶), 但是失去一个明确目标后,只能分散的到处尝试这种感觉还是很难过的, 尤其是你的研究方向和平时用到的东西基本上是完全不相干的\u0026hellip;(当然我承认网络系统方向这种已经发展了这么多年, 一定是很稳定了, 难以快速入门也很正常吧)\n然后这一年里一直沉迷于一款游戏, Klei的缺氧, 真的是很上头，从一开始刚买算起已经一年半了, 成功送走了我的笔记本电脑..把硬盘给差点玩坏了\n这个游戏里面有很多内容就是模块化与原始之间的平衡trade-off, 看你是休闲玩家愿意动手点还是一定要通过自动化实现一切(完全的解放人力, 但是需要设计的非常合理以及包含各种异常情况, 有足够的容错空间, 容易建造(你总不能开着debug玩游戏吧xs)) 入门之后, 这个游戏就变成了一种数学计算, 当你发现各种物质的循环已经可以满足生存需要之后就可以删档重开了; 每次重开都是完全不同的世界和资源分布, 我想这也是为什么我可以开了10几个存档玩同一种类型图的原因吧, 因为一个很小的扰动也可以使得开局的顺序完全改变, 最急迫需要解决的总是最优先的, 比如水源, 水在这个游戏内就是一切(或者说最根本的能源物质, 因为其他的产物都可以从这里获取), 水可以换取石油,石油等于充足的电力, 电力供应使得水的电解可持续,氧气问题也迎刃而解, 最后有了电力就可以合理控温, 种植也没有了难度, 食物问题即使你不做永久保鲜也可以完全解决. 这里之后生存就毫无难度，就可以尝试各种新思路了, 因此你可以看到每一局游戏除了这前面这些常规套路之外, 套路之外的世界才是最有意思的, 可以设计自己的模块来尝试一下，看各种模块实现的基本原理也是一种很有意思的体验(你可以发现原理有时并不复杂，但是你可能很久也不会发现) 这其实和很多数学(算法)题类似，有的时候这些算法真的就是很简单的原理，但是用的很好，或者是一个很复杂的模型去降维打击一个复杂问题；前者在我看了是比后者要有趣的多的，也是追求的目标吧。。。\n今年读的书感觉很杂, 看了系统设计, python语言, 读了一遍CLRS, 算法4还有之前一直没有理解的统计学习方法的监督学习部分, 也重新研究了网络设计(黑皮书看着很难的样子, 实际上只是话多而已(确实是因为网络很多东西都是人自己定义的协议, 这些内容就是很麻烦, 有很多的边界情况和实际的调整, 掌握大概就ok)), cs144的测试实现我感觉也是很有趣值得去做的(当然lab都是对于想学习的人都是很有意思的), 至于很火的6.824很遗憾我忘记做了, 以至于到现在都是map-reduce(悲), 23年肯定做这个嗷, 昨天还被b站推送到了raft的实现\u0026hellip;\n一个人所属的环境 \u0026raquo; 一个人的taste \u0026gt; 一个人的智商 \u0026gt; 一个人的能力 \u0026gt; 一个人的语言文字 \u0026raquo; 一个人说了什么\n2022是真的可怕, 23别的不说一定是很混乱的, 会不会继续可怕我觉得是的, 等到一年后坐等打脸吧。 有很多东西都不见了\u0026hellip; # ","date":"2022 年 12 月 31 日","externalUrl":null,"permalink":"/posts/2022/2022-sum/","section":"Posts","summary":"","title":"可能是2022的年度总结","type":"posts"},{"content":"","date":"2022 年 12 月 31 日","externalUrl":null,"permalink":"/tags/%E6%91%B8%E9%B1%BC/","section":"Tags","summary":"","title":"摸鱼","type":"tags"},{"content":"","date":"2022 年 12 月 31 日","externalUrl":null,"permalink":"/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","section":"Tags","summary":"","title":"年度总结","type":"tags"},{"content":"","date":"2022 年 12 月 31 日","externalUrl":null,"permalink":"/categories/%E6%80%BB%E7%BB%93/","section":"Categories","summary":"","title":"总结","type":"categories"},{"content":"","date":"2022 年 12 月 30 日","externalUrl":null,"permalink":"/tags/networks/","section":"Tags","summary":"","title":"Networks","type":"tags"},{"content":"先看最简单的跨主机通信组件的实现(Flannel), 后面再补上其他的实现.\n基于Flannel组件的第3层流量转发, 解决 不同主机之间 跨机通信的问题(网络资源的问题) 它自己的简介:\n每个主机上运行一个简易的代理程序 flanneld\n每个主机上的代理程序负责分配一组子网段, 来源于 主机集群所属的 网络地址空间;\nFlannel runs a small, single binary agent called flanneld on each host, and is responsible for allocating a subnet lease to each host out of a larger, preconfigured address space. Flannel uses either the Kubernetes API or etcd directly to store the network configuration, the allocated subnets, and any auxiliary data (such as the host\u0026rsquo;s public IP). Packets are forwarded using one of several backend mechanisms including VXLAN and various cloud integrations.\n举个例子：一个集群里面有两台主机(实际的机器A和B)\n集群范围内的网络地址空间为10.1.0.0/16(CIDR)\nA主机获得的子网IP范围10.1.15.0/24 A主机上有两个pod, pod1和pod2, 对应的IP分别是10.1.15.2/24和10.1.15.3/24\nB主机获得的子网IP范围10.1.16.0/24 B主机上有两个pod, pod3和pod4, 对应的IP分别是10.1.16.2/24和10.1.16.3/24\n如果 A主机上的pod1 想要与B主机上的pod3进行通信, 需要进行某种转发才能完成(因为不在一台机器上)\nA主机的flanneld程序会把自己的subnet信息存入etcd: subnet = 10.1.15.0/24的所在主机可以通过一个内网IP 192.168.0.100访问;\n同样的B主机上的flanneld程序会把自己的subnet信息存入etcd: subnet = 10.1.16.0/24 的所在主机可以通过一个内网IP 192.168.0.200访问;\n每台主机上的flanneld通过监听etcd，也能够知道其他的subnet与哪些主机相关联;\nflanneld只要想办法将封包从Machine A转发到Machine B; 一旦目的地址为10.1.16.2/24的数据包可以到达主机B, 就能够通过CNI0网桥转发到这台主机上的pod3, 从而达到跨机器通信.\n采用的方法可以有很多种:\nhostgw: 把B主机看作一个网关, 当有封包的目的地址在subnet 10.1.16.0/24范围内时，就将其直接转发至B即可; 在满足仍有subnet可以分配的条件下，我们可以将上述方法扩展到任意数目位于同一子网内的主机; 每台主机都是租借了一个subnet，如果到了一定时间不进行更新，那么该subnet就会过期从而重新分配给其他的主机(类似于DHCP)\n调用GetNetworkConfig()\nSubnetLen表示每个主机分配的subnet大小;如果集群的网络地址空间大于/24，则SubnetLen配置为24，否则它比集群网络地址空间小1，例如集群的大小为/25，则SubnetLen的大小为/26\nSubnetMin是集群网络地址空间中最小的可分配的subnet，可以手动指定，否则默认配置为集群网络地址空间中第一个可分配的subnet。例如对于10.1.0.0/16，当SubnetLen为24时，第一个可分配的subnet为10.1.1.0/24\nSubnetMax表示最大可分配的subnet，对于”10.1.0.0/16″，当subnetLen为24时，SubnetMax为”10.1.255.0/24″\nBackendType为使用的backend的类型，如未指定，则默认为UDP\nBackend中会包含backend的附加信息，例如backend为vxlan时，其中会存储vtep设备的mac地址\n发现新节点\n当本主机的flanneld启动时，如果集群中已经存在了其他主机，我们如何通过backend进行配置，使得封包能够到达它们 如果之后集群中又添加了新的主机，我们如何获取这一事件，并通过backend对配置进行调整，对于删除主机这类事件同理 通过etcd解决\nhostgw是最简单的backend，它的原理非常简单，直接添加路由，将目的主机当做网关，直接路由原始封包。\n我们知道当backend为hostgw时，主机之间传输的就是原始的容器网络封包，封包中的源IP地址和目的IP地址都为容器所有。这种方法有一定的限制，就是要求所有的主机都在一个子网内，即二层可达，否则就无法将目的主机当做网关，直接路由。\n而udp类型backend的基本思想是：既然主机之间是可以相互通信的（并不要求主机在一个子网中），那么我们为什么不能将容器的网络封包作为负载数据在集群的主机之间进行传输呢？这就是所谓的overlay\n另外两个著名的k8s网络策略插件,Calico和Cilium后面(一定介绍)(下次一定\u0026hellip;)\n还需要补充一些k8s网络模型的相关知识, CNI的工作原理; 期望可以通过本地搭建minikube的方式体验k8s不同service之间的网络通信方式(service是k8s管理的服务对象, 而一个服务往往可以包括多个pod(多备份), 不同的pod可以处在不同的node上, node依附在物理机上, 而物理机上的IP地址,网络地址都是可能不同的, 因此是可能出现从一个pod到另一个pod的网络通信; 如何更好的实现这一点? 需要我们在CNI上面进行自己定义, 通过CNI提供的网络资源相关接口, 适配服务从而实现相应的功能)\n","date":"2022 年 12 月 30 日","externalUrl":null,"permalink":"/posts/2022/k8s-network/","section":"Posts","summary":"","title":"不同的网络结构整理","type":"posts"},{"content":"暂时没想好缩略图里面放什么比较好, 目前的体验还挺好, 每天早上起来都可以脑子被暴击一次:(\n被网络和算法双重暴击\u0026hellip; 真的好工程啊(T)\n12.26 ABC 269F # 给你一个特殊的矩阵, 每个格子的下标为$(i,j)$, 若$i+j$是一个偶数, 那么这个矩阵格子数值为0, 否则为$i*M + j$, 给你$Q$个查询, 每组查询包含$r_1, r_2, c_1, c_2$;\n求 $r_1 \\leq r \\leq r_2$, $c_1 \\leq c \\leq c_2$ 之间的所有数字之和. 关键:\n看出间隔两行之间的和是一个等差数列关系, 公差是固定的.\n每一个query都可以看作两组基向量(偶数行, 奇数行)所组成的等差数列, 然后对这两个数列分别求和就可以了.\n注意$c_1$还有$c_2$的范围限制, 确定对应的格子内取值, 然后求出单行的数值, 最后套公式求和即可。\n12.27 ABC 270F # https://atcoder.jp/contests/abc270/tasks/abc270_f\n每一个节点有以下三种选择\n在该节点 建造港口 $X$ 在该节点 建造机场 $Y$ 在指定两节点间 建造一条公路 $(x, y)$ 代价为$z$ 处理1,2两种情况 =\u0026gt; 建立超级汇点, 让节点与这个虚拟节点连边, 最后两个节点相连 \u0026lt;=\u0026gt; 引入超级节点后这两个节点相连 (通过3直接连接, 或者是通过两次操作1进行连接)\n最后考虑使用情况: 1, 2, 12, 不使用12这4种情况都需要考虑, 每一次执行一次至多$2N+M$条边的最小生成树算法, 因此时间复杂度是ok的。\n12.28 ABC 268F # https://atcoder.jp/contests/abc268/tasks/abc268_f\n邻项交换(假设相邻两项 $P_i$, $P_{i+1}$) $P_i$ 有$X_i$个X, 数字总和为$Y_i$ $P_{i+1}$ 有$X_{i+1}$个$X$, 数字总和为 $Y_{i+1}$ 那么$P_i$放在$P_{i+1}$前面, score = $X_i \\times Y_{i+1}$ + ($P_i$内部分数) + ($P_{i+1}$内部的分数)\n如果$P_{i+1}$放在$P_i$前面, score = $X_{i+1} \\times Y_i$ + ($P_i$内部分数) + ($P_{i+1}$内部的分数)\n那么交换更优秀 =\u0026gt; $X_i \\times Y_{i+1} \u0026lt; X_{i+1} \\times Y_i$\n所以按照这个顺序排序后得到的结果就是最优秀的，我们按照这个顺序排序并一次求解即可。复杂度就是这一次排序，注意不要爆int了\n12.29 ABC 267F # https://atcoder.jp/contests/abc267/tasks/abc267_f\n树上两点之间的距离查询问题\n给定一颗树和树上的一点u, 问与u距离值为k的点有哪些? 和树的直径有什么关系?\n直径端点L和R对应的距离是树上所有点对之间最大的。 只有在范围内部的才有可能存在 因此 只需要考虑从直径的端点出发的路径组成, 就能找到这些节点的距离为k的答案节点(因为直径路径是树上最长的路)\n为什么需要从两个端点都走一次?\n因为存在一些点, 当我们第一次遍历到的时候, 发现路径长度小于深度k(因此没有答案节点) 但是从另一侧过来就可以满足答案要求的, 所以需要从直径两端各自遍历一次. 12.30 259F Select-Edges # https://atcoder.jp/contests/abc259/tasks/abc259_f\n挑选树的边, 满足度数约束的情况下尽可能取得最大值, 求这个最大值;\n最大值 =\u0026gt; 最优化问题 =\u0026gt; 贪心 或者 dp 由于存在度数的约束 =\u0026gt; 每一条边关联两个端点, 无法简单的按照边权大小贪心 =\u0026gt; 会想到dp =\u0026gt; 树形dp 树形dp, 特点就是可以按照子树的方式考虑子问题的答案, 然后根据树的特质进而进行状态转移求解。\n考虑一个节点$u$, 以及关联$u,v$的一条边, 假设它具有的边权为$w$ 如果此时节点$u$, 已经选了一些边, 但是度数大小小于 $d[u] - 1$那么可能需要选这条边, 从而以节点$u$为根的子树的价值就增加 $S[v] + w$; 如果度数达到了上限, 就不选.\n假设节点$u$, 有$v_1, v_2, \u0026hellip;, v_n$这些节点关联, 那么这些子树挑选哪些会是最好的? 那就是子树权重和(度数未满) + (u,v_i)边的权重 - 不选这条边的最大权重和(v_i度数已经满了)的差值大于0的那些节点. 按照这个顺序排序子树节点, 贪心选取前$d[u]$个(如果差值为负则直接退出) 实现算法的瓶颈在于排序的复杂度, 最终是$O(NlogN)$的.\n参考实现:\n","date":"2022 年 12 月 29 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1226/","section":"Posts","summary":"\u003cp\u003e暂时没想好缩略图里面放什么比较好, 目前的体验还挺好, 每天早上起来都可以脑子被暴击一次:(\u003c/p\u003e\n\u003cp\u003e被网络和算法双重暴击\u0026hellip; 真的好工程啊(T)\u003c/p\u003e","title":"灵茶の试炼-1226","type":"posts"},{"content":"补档ing\n12.19 ABC 272E # https://atcoder.jp/contests/abc272/tasks/abc272_e\n输入 n(≤2e5) m(≤2e5) 和长为 n 的整数数组 a (-1e9≤a[i]≤1e9)，下标从 1 开始。 执行如下操作 m 次： 对 1~n 的每个 i，把 a[i] += i。 每次操作后，输出 mex(a)，即不在 a 中的最小非负整数。\n可以知道的一点是mex(a)这个答案的范围一定在$[0,N-1]$之内, 因为假设我们操作很多次之后所有位置上的数字都会变得很大, 由于只有N个数字, 根据鸽笼原理, 至多覆盖从$0$开始的N个位置, 那么如果让最小的那个数字超过了0, 答案就可以取到0, 依次类推, 最终可以发现答案取值是有一个很小的范围的.\n那么我们对于每个位置上的数字进行累加次数的讨论, 感兴趣的只有:\n累加L次后, 这个数(如果是负数)刚好大于0了 累加R次后, 这个数刚好超过(大于等于N)了 那么在$[L,R]$区间内部的累加次数, 这个位置上的数字就处于影响范围内, 需要被考虑; 我们就根据这样的方式按照累加次数进行归类, 每次都来验证一下哪个数字没有出现在$[0,N-1]$的范围内\n(为什么不会超时?) 第一个数字+1, 累加次数至多N次, 出现N次 第二个数字+2, 累加次数至多N/2次, 出现N/2次 \u0026hellip; 一直到第N个数字+N, 只会加N/N=1次, 出现1次 $\\sum_{i=1}^N \\frac{N}{i}$这个级数的模糊上界至少是$N \\times log(N)$的, 因此不用担心暴力验证会出现超时的问题(循环内部的执行操作次数最多只有NlogN次).\n{% asset_img 272E.png 272E %}\n12.20 ABC 266F # https://atcoder.jp/contests/abc266/tasks/abc266_f\n输入 n (3≤n≤2e5) 和 n 条边，点的编号在 [1,n] 内，表示一个没有重边和自环的无向连通图。 然后输入 q(≤2e5) 表示有 q 个询问，每个询问输入两个数 x 和 y (1≤x\u0026lt;y≤n)。 对于每个询问，如果 x 和 y 之间只存在唯一的简单路径，则输出 Yes，否则输出 No。\n没写, 等填坑, 基环树\n12.21 ABC 275F # https://atcoder.jp/contests/abc275/tasks/abc275_f\n输入 $n(\\leq 3000) m(\\leq 3000)$ 和长为 $n$ 的数组 $a (1\\leq a[i] \\leq 3000)$。 每次操作你可以删除 $a$ 的一个非空连续子数组。 定义 $f(x)$ 表示使 $sum(a) = x$ 的最小操作次数。 输出 $f(1), f(2), \u0026hellip;, f(m)$\n构造一个系数数组$b[n]$ 那么当我们选取某个$a[i]$的时候, 对应$b[i] = 1$, 否则$b[i] = 0$, 表示不取$a[i]$\n考虑$S = \\sum_{i=1}^N a[i]b[i] $, 当$S = x, x\\in [1, m]$时, 等价于求出了原问题的一种操作方式\n定义$f[S][0/1]$表示前?个数字, 取和为$S$, 最后一个数字取or不取的最少操作数.\n类似于0-1背包问题, 我们发现:\n当前和为S, 这个数不取的情况, 等于之前取+不取(删除, 1次) $f[S][0] = min(f[S][0], f[S][1] + 1)$ 当前和为S, 如果这个数可以取 $f[S][1] = min(f[S - x][0], f[S - x][1])$ , 否则$f[S][1] = inf$ 最后返回 $S = 1 - M$ 的所有情况解的值即可.\n12.22 ABC 273E # https://atcoder.jp/contests/abc273/tasks/abc273_e\n一开始你有一个空数组 a 和一个 1e9 页的笔记本，每页上都记录着一个空数组。 有四种类型的操作： ADD x：在 a 的末尾添加元素 x (1≤x≤1e9)。 DELETE：如果 a 不为空，删除 a 的最后一个元素。 SAVE y：把 a 记在第 y 页上（覆盖原来的数组）。 LOAD z：把 a 替换为第 z 页上的数组。\n输入 q(≤5e5) 和 q 个操作。 在每个操作结束后，你需要输出 a 的最后一个元素（数组为空时输出 -1）。\n想到前向链表/前缀操作树就很容易实现了(因为我们发现只关心最后一次操作的结果, 之前的数据实际上不需要我们去进行额外的维护, 只要结构可以不断快速的累加, 复制就可以满足条件了)\n12.23 ABC 271E # https://atcoder.jp/contests/abc271/tasks/abc271_e\n输入 $n,m,k (\\leq 2e5)$, 然后输入 $m$ 条边，每条边输入两个点 $x\\ y$（表示从 $x$ 到 $y$ 的一条有向边，点的编号 $1-n$ 和一个值在 $[1,1e9]$ 内的边权，每条边的编号 $1-m$\n图中没有自环，但可能有重边。\n然后输入一个长为 $k$ 的数组 $a (1 \\leq e[i] \\leq m)$\n找到一条从 $1$ 到 $n$ 的路径，满足路径上的边的编号是 $a$ 的子序列\n输出满足这个要求的路径的最短长度。如果不存在, 输出 $-1$。\n找到一条从 $1$ 到 $n$ 的路径, 满足路径上的边的编号是 $a$ 的子序列: 实际上这个条件还是很强的一个约束(必须按照序列的顺序走, 还需要能够到达最终的终点) 最短长度 =\u0026gt; 通过类似于dijkstra的方式放缩这个序列上的所有边, 加边修改距离, 直到序列内所有的边都被放缩使用过, 观察终点的距离值是否可以到达? 类似于这样的放缩:\nfor e in seq: from, to, w = e if dis[to] \u0026lt; dif[from] + w: dis[to] = dis[from] + w ","date":"2022 年 12 月 25 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1219/","section":"Posts","summary":"","title":"灵茶の试炼-1219","type":"posts"},{"content":"构造与贪心\n12.12 ARC 147B # https://atcoder.jp/contests/arc147/tasks/arc147_b\n输入 $n(\\leq 400)$ 和一个 $1-n$ 的排列 $p$, 下标从 $1$ 开始。 你有两种操作： 操作 \u0026ldquo;A i\u0026rdquo; 可以交换 p[i] 和 p[i+1] ; 操作 \u0026ldquo;B i\u0026rdquo; 可以交换 p[i] 和 p[i+2]。\n你需要让 $p$ 变为递增的。 最小化操作 $A$ 的次数，同时总操作次数不能超过 $1e5$。 输出总操作次数，以及具体的操作内容，详见样例。 如果有多个符合要求的方案，输出任意一种。 保证存在这样的操作方案。\n注意到B操作不会改变元素下标的奇偶性,因此无论B操作运用多少次都没办法消除相邻的逆序对, 我们可以通过B操作把元素按照奇数下标序列, 偶数下标序列的方式进行排序(这是肯定可以做到的,次数的上限就是2 * 200 * 200 = 40000次)\n然后这里还会存在相邻元素之间的逆序关系\n奇数下标序列: $p_1, p_3, \u0026hellip;, p_{2k+1}$\n偶数下标序列: $p_2, p_4, \u0026hellip;, p_{2k+2}$\n此时我们再用A操作交换即可, 次数也至多只有400次; 最后就可以得到答案.\n12.13 ARC 125C # https://atcoder.jp/contests/arc125/tasks/arc125_c\n输入 $n, k (k\\leq n \\leq 2e5)$ 和一个长为 $k$ 的严格递增数组 $a$，元素范围 $[1,n]$。 输出一个 $1-n$ 的排列 $p$，使得 $a$ 是 $p$ 的一个 $LIS$。 如果有多个这样的 $p$，输出字典序最小的那个。 注：LIS 指最长上升子序列。\n先考虑LIS的构成, $x_1,\u0026hellip;,x_k$ 首先我们的序列肯定要包含这些元素, 接着不能够让LIS变得更长\n所以, 比$x_k$还要大的元素 要么放在最前面, 但是这样会使得字典序排序很大, 要么就预留最后一个元素$x_k$放在最后面, 中间把这些元素倒序排列, 这样可以让字典序排序更靠前一点.\n解决完这些元素之后, 为了使得字典序最小, 需要尽可能地把小元素插入间隙中, 这一点可以直接通过一个指针完成, 每次考虑插入比当前已经放入元素更小, 但是还没有被使用地数字.\n12.14 ARC 91C # https://atcoder.jp/contests/arc091/tasks/arc091_c\n输入 $n, a, b(\\leq 3e5)$ 构造一个 $1-n$ 的排列，其 LIS 的长度为 $a$，LDS 的长度为$b$。 如果不存在这样的排列，输出 $-1$, 否则输出任意一个满足要求的排列。 注：LIS 指最长上升子序列, LDS 指最长下降子序列。\n可以选择其中的一种序列进行构造, 例如长度为$a$的LIS, 可以构造成$X_1= 1,2,3,\u0026hellip;,a$, $X_2=a+1,a+2,\u0026hellip;, 2a$这样的片段, 然后组成$X_2, X_1$这样的排列就可以构造出LDS = 2的下降子序列了.\n大致的实现方式就是这样的, 具体在实现是可以通过最终剩下元素的数量直接判断是否可能达成LDS/LIS的构造, 提前结束.\n贴一个实现:\n12.15 keyence2020 D # https://atcoder.jp/contests/keyence2020/tasks/keyence2020_d\n输入 $n \\leq 18$ 和两个长为 $n$ 的数组 $a, b$，元素范围在 $[1,50]$\n$a[i]$ 和 $b[i]$ 表示第 $i$ 张牌的正面数字和背面数字。 初始所有牌正面朝上。 每次操作你可以交换第 $i$ 和 $i+1$ 张牌的位置，同时翻转这两张牌 输出让看到的数字从左往右是递增（允许相等）所需要的最小操作次数。 如果无法做到，输出 $-1$\n12.16 ABC 214E # https://atcoder.jp/contests/abc214/tasks/abc214_e\n输入 $t(\\leq 2e5)$ 表示 $t$ 组数据，每组数据输入 $n(\\leq 2e5)$ 和 $n$ 个区间 $[L,R]$，范围在 $[1,1e9]$ 所有数据的 $n$ 之和不超过 $2e5$\n你有 $n$ 个球，第 $i$ 个球需要放在区间 [L,R] 内的整数位置上, 但每个整数位置上至多能放一个球; 如果可以做到, 输出 Yes, 否则输出 No\n每个整数位置上只能放一个球, 所以需要合理安排这个球所对应的区间, 让它尽可能满足那些更难以被靠后的位置满足的区间, 这样看起来是最好的.\n所以, 这样就可以想到贪心的做法, 首先排序这些区间(升序), 然后对于左端点相同的区间, 我们排序它们的右端点, 右端点最小的优先被我们考虑放球\n如果此时这一系列区间都被处理完了, 可以直接(放到左端点)\n否则放在这个端点上面, 并且与 此时放球的指针位置 进行比较, 如果当前这个区间在左侧, 说明不可能完成了, 直接退出; 否则就在这个位置上模拟放球, 增加指针\n不断重复这样的操作, 直到所有的区间被处理完成或者是无法完成中途退出.\n实际的实现中用一个set来维护我们的放球位置, 首先初始化为所有的左端点+哨兵maxn\n","date":"2022 年 12 月 25 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1212/","section":"Posts","summary":"\u003cp\u003e构造与贪心\u003c/p\u003e","title":"灵茶の试炼-1212","type":"posts"},{"content":"","date":"2022 年 12 月 15 日","externalUrl":null,"permalink":"/categories/%E6%9C%AC%E7%A7%91%E8%AF%BE%E7%A8%8B/","section":"Categories","summary":"","title":"本科课程","type":"categories"},{"content":"","date":"2022 年 12 月 15 日","externalUrl":null,"permalink":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"Tags","summary":"","title":"机器学习","type":"tags"},{"content":"CNN,RNN, Transformer原理; 无监督学习\nCNN # conv卷积的意义\n全连接to卷积操作 卷积可以看作局部全连接-局部感知眼-局部的特征 multi-kernel pooling操作的意义\nsubsampling: 采样降低数据的维度 扩大局部感受区域的范围 提高translation invariance 全连接层\n特征提取到分类输出的连接 激活函数的选取\nsigmoid tanh relu(防止梯度消失, 稀疏激活,计算简单) CNN的关键操作\n卷积 非线性 空间区域的pooling操作 特征提取 filter: 过滤器(滤波器)\n大小假设为$W\\times H\\times D$ 计算经过该滤波器卷积后的数据尺寸 滤波器的参数数量(WHD) 低级特征-高级特征-最终特征\nfilter的步长选择\n空间性 stride\u0026gt;1的时候可能无法完全覆盖-加padding Pooling层\n表征大小更小可控 每一步激活函数之后进行独立操作 max-pooling 常见的取值：F=2,S=2或者F=3,S=2\n高级CNN网络 # Residual-Network 残差网络\n如果输入特征是理想的 如果映射是理想的，最终的响应会怎样变化？ H(x) = x H(x) = F(x) + x F(x) = H(x) - x 当激活函数为0(F(x) = 0), H(x) = x\n对于图片数据而言(原图片x - 变化后的图片F(x))之间的差异; 残差块Residual-Block机制\n1*1 conv的使用, 减少参数数量 其他的网络结构：\n多分支Multi-branch Inception 分组卷积 Ensemble Channel-Attention机制 Conv-Block的attention shared-mlp Attention机制 # 注意力机制\n快速提取局部重要特征 不同类型的attention\n注意力机制\n关心的特征是$q$, 目前的所有特征记作$X=(x_1,\u0026hellip;,x_N)$, $s(x_n,q)$是打分函数, 有不同的选择\n计算注意力分布$\\alpha = p(z = n|X,q) = softmax(s(x_n,q))$\n根据注意力关于输入信息的分布计算加权平均值作为最终的attention大小\n$att(X,q) = \\sum_{n=1}^N \\alpha_n x_n$\nhard-attention 键值对注意力 用$(K,V)$标识N个输入信息\n$att((K,V),q) = \\sum_{n=1}^N \\alpha_n v_n$ multihead-attention 并行从输入信息中选取多组信息, 每个attention关注输入信息的不同部分。 $att((K,V),Q) = att((K,V), q_1) \\oplus \u0026hellip; \\oplus att((K,V), q_M)$ $Q = [q_1, \u0026hellip;, q_M]$\nglobal-attention=soft-attention 全局注意力机制\nlocal-attention 局部注意力\n计算attention的过程\n$e_i = \\alpha(u, v_i)$ 计算attention分值 $\\alpha_i = \\frac{e_i}{\\sum_i e_i} $ normalize这些分数 $c = \\sum_i \\alpha_i v_i$ 进行编码 key-value, query output\nSelf-attention机制 # 问题：如何建立非局部的依赖关系\n全连接模型的局限性：输入长度变化就无法处理了 自注意力模型：通过attention机制动态生成参数 对self-attention来说，它跟每一个input vector都做attention，所以没有考虑到input sequence的顺序\n类似于键值对Attention, self-attention将每一个输入的向量都做attention $$ Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt d_k })V $$\n$\\alpha = QK^T$ softmax $\\alpha$得到对应的系数 乘以$V$得到self-attention的分值 动态更新 与CNN的区别-提取特征：\n全局的感知-对比-卷积结构的感知\n$Q = XW^Q$, $V = XW^V$, $K = XW^K$ 其中 $W^Q, W^V, W^K$ 都是可以动态变化的参数矩阵\n不同类型之间的attention计算:\n2-head的attention(取两个输入同时加和做attention计算) self-attention机制的问题\n没有位置信息 解决：引入位置编码 Positonal Encoding 每个位置都有一个独特的位置向量标记 $e^i$ 最终将扩展的attention-score记作 $e^i + a^i$, 其中 $a^i = f(q^i, k^i, v^i)$\nCV中Attention机制的应用 # 以CBAM为例：通道注意力机制\n按照通道进行处理得到通道特征，进行excitation 输出 = 每个通道分配的权重 =\u0026gt; 重新定义输入的不同通道之间的重要性 =\u0026gt; Scale系数\n一组特征在上一层被输出，这时候分两条路线，第一条直接通过，第二条首先进行Squeeze操作（Global Average Pooling），把每个通道2维的特征压缩成一个1维，从而得到一个特征通道向量（每个数字代表对应通道的特征）。然后进行Excitation操作，把这一列特征通道向量输入两个全连接层和sigmoid，建模出那为什么要认为所有通道的特征对模型的作用就是相等的呢？ 特征通道间的相关性，得到的输出其实就是每个通道对应的权重，把这些权重通过Scale乘法通道加权到原来的特征上（第一条路），这样就完成了特征通道的权重分配。（依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。）\nTransformer # seq2seq的任务：序列输入，序列输出 encoder-decoder模型\nencoder接受输入序列信息，并生成一个中间结果 decoder读取中间结果进行decoder，产生结果 encoder/decoder的内部可以是CNN/RNN\ntransformer的encoder结构：输入- 输入嵌入编码-多头注意力机制+残差+归一化 + 前向MLP +残差+归一化- 最终的encoder结果 decoder\nauto-regressive\noutput-embedding\n解码器结构的transformer\nmasked-多头注意力机制+add\u0026amp;norm 多头注意力机制+add\u0026amp;norm feed-forward add\u0026amp;norm 线形层 softmax 输出概率分布 为什么需要掩码操作？ 解码的结果会影响下一个序列元素的结果, Q会发生变化! 不知道正确的输出长度，所以需要增加终止符号\ntransformer的结构中增加了encoder到decoder的交叉注意力机制\nEmbeding # embed层的作用：如果采用稀疏的one-hot编码变成稠密的向量表示 LLE(Local Linear Embedding)\n局部线性embed：用$w_{ij}$ 表示数据点$x_i$ 和数据点$x_j$之间的关系程度 寻找一组合适的权重$w$ 满足$\\sum_{i} |x^i - \\sum_{j} w_{ij} x_j |$ 转变为固定$w_{ij}$, 寻找一组$z_i$满足上述的最小化约束 =\u0026gt; 得到局部2的嵌入压缩 Laplacian Eigenmaps\nt-SNE\nt分布的随机邻居嵌入算法 相似的数据靠近，但是不同的数据可能覆盖同一个区域 计算组之间的相似度$S(x_i, x_j)$\n数值对之间的相似度 $S\u0026rsquo;(z_i, z_j)$\n然后寻找一组$z$系数使得两类分布尽可能靠近\n$$ L = \\sum_i KL(P(|x_i)||Q(|z_i)) $$\n无监督学习 # Clustering # 聚类算法\n模板：\n寻找相似点集合 寻找自然的描述数据的属性 寻找合适的分组方式 检测异常点/outlier检测 特征提取/选择 最小化 类内部的差异程度(最小化协方差矩阵) 最大化 观测样本在预测分布下的似然程度\n实现这一类的算法需要满足以下要求：\n可扩展性 可以处理不同的数据类型 可以处理任意形状的聚类 尽可能少的领域知识 能够处理噪声和异常点 对于输入数据的顺序不敏感 维度高 可解释性，可用性 分类：\n分类-聚类 完全不重合的聚类 K-means 存在重合的聚类 fuzzy K-means 概率聚类 高斯混合分布 分层-聚类（结构化） 相似度指标\n数值属性 按照距离的类别划分指标：\nMinkowski指标(Manhattan距离, Euclidean距离等等) 最小间距 最大间距 平均间距 类别指标\n自底向上 合并(linkage) 自顶向下 分割(split) Kmeans 目标： 最小化$J$\n$$ J = min \\sum_{i=1}^n \\sum_{k=1}^K r_i^(k) |x_i - \\mu_k|^2 $$\n{% raw %}\n$$ r_i^{(k)} = \\begin{cases} 1,\\ if\\ x_i 属于第k类 \\ 0,\\ 其余的情况 \\end{cases} $$\n$r_i^{(k)} = 1\\ 如果x_i属于第k类,\\ 否则为 0$ $\\mu_k$ 是分为第k类的聚类的原型向量 {% endraw %}\nKmeans的工作原理就是将$n$个数据点放入$d$维空间的$K$个聚类中, 满足最小化$J$的约束。\n如何找到这样一种划分?\n$r_i$ $\\mu_k$ 迭代步骤：\n固定 $\\mu_k$, 最小化J, 得到 $r_i$ 固定 $r_i$, 最小化J, 得到 $\\mu_k$ K-means是基于以上两步迭代的算法 不断重复以上步骤, 直到赋值不再改变 更新 $r_i^{(k)}$ 的过程和更新 $\\mu_k$ 的过程\n$\\mu_k$ 的更新 对于 $\\frac{\\partial J}{\\partial \\mu_k} = 0$ 求解得到 $\\mu_k = \\frac{\\sum_i r_i^{(k)}x_i }{\\sum_i r_i^{(k)} }$\n$\\mu_k$ 是被分为第k类数据点的均值, 因此这个算法称为k-means\n算法的执行过程：设置两个部分的迭代次数\n$E$ 次更新 $r_i$ 系数 适配数据点 $x_i$ 到最近的聚类 $k$ $M$ 次更新 $\\mu_k$ 均值大小, 重新划分聚类组织方式 K-means算法的应用:\n有损失的数据压缩 存储K个聚类的中心数据, 而不是原始的数据点(codebook的实际原理) 每一个数据点用K聚类来近似, 从而起到了压缩数据的功能(K \u0026laquo; N的情况下) K-means的问题：\n对于离群点非常敏感 改进1：K-medoid算法(中心点聚类)\n由中位数改为广义距离函数 $J\u0026rsquo; = min \\sum_{i=1}^n \\sum_{k=1}^K \\nu (x_i, \\mu_k) $ 中位数不一定是K类中真实存在的元素; 而中心点是一定存在的 改进2: soft K-means算法\n算法有额外的一个参数$\\beta$ 标识分布的陡峭程度 $\\beta$ 是方差的倒数关系, $\\sigma = \\frac{k}{\\sqrt \\beta }$ 更新$r_i^{(k)}$的算式\n点$x_i$关于聚类$k$的归属程度 $r_i^{(k)} = \\frac{exp(-\\beta d(\\mu_k, x_i))}{\\sum_k\u0026rsquo; exp(-\\beta d(\\mu^{k\u0026rsquo;}, x_i))}$ 求和发现始终等于1\n算法的执行过程：\n不断更新 $\\mu_k = \\frac{\\sum_i r_i^{(k)} x_i}{R^{(k)}}$ 以及总和 $R^{(k)} = \\sum_i r_i^{(k)}$ 区别：\n$r_i^{(k)}$的取值变成了[0,1]区间 Hard K-means算法中的 min去除 Soft K-means 用求取最大归属程度$r_i$的方式找最佳划分 混合高斯分布MoG # 版本1\n每一个聚类看作是一个球形高斯分布，拥有一个自己的半径 $\\beta^k = \\frac{1}{{\\sigma_k}^2 }$\n算法自动更新高斯分布的半径相关参数$\\sigma_k$ 算法也包含了每个聚类的权重系数$w_1,\u0026hellip;,w_k$ 最大似然估计 多个高斯分布混合的情况 初始化：\n随机初始{\\mu_k}$ E step: 计算归属度 $r_i^k = \\frac{\\pi_k N(x|\\mu_k, \\sigma_k)}{\\sum_j \\pi_j N(x|\\mu_j, \\sigma_j)}$ M step: 更新$\\mu_k$, $\\sigma_k^2$, $\\pi_k = \\frac{R^k}{\\sum_k R^k}$, $\\sum_{k=1}^K \\pi_k = 1$ 目标函数 $I(X) = ln P(X|\\pi, \\mu, \\sigma) = \\sum_{i=1}^n ln(\\sum_{k=1}^K \\pi_k N(x_i|\\mu_k, \\sigma_k)) $ 最大似然估计\n每个数据点的概率密度由混合高斯分布模型给出 $p(x|\\theta) = \\sum_{k=1}^K \\pi_k p(x|\\theta_k) $ 聚类算法的评价标准：\npurity 纯度 $I(\\Omega, C) = \\frac{1}{N} \\sum_k max_j |w_k \\cap c_j|$\nNMI 互相之间信息的依赖程度\nKL散度 $KL(p(x,y) || p(x)p(y))$ MI最小值0, 最大值维完美划分(但是可以继续细分) 引入 $NMI(\\Omega, C) = \\frac{I(\\Omega, C)}{(H(\\Omega) + H(C)) / 2}$ Rand Index\n$RI = \\frac{TP+TN}{TP+TN+FP+FN}$ TP,TN,FP,FN F系数\nRI对于FP和FN给了相同的权重 F系数对于FN更大的惩罚系数$\\beta \\gt 1$ $F_{\\beta} = \\frac{(\\beta^2 + 1)PR}{\\beta^2 P + R}$ 其中 $P = \\frac{TP}{TP+FP}$, $R = \\frac{TP}{TP+FN}$\nPCA降维 # PCA主成分分析, 寻找的是最大方差的降维结果组成 以1维为例： $z_1 = w_1 \\dot x$\n可以看作是把x投影到w的平面上, 获得一组集合$z_1$\n目标：$Var(z_1) = \\frac{1}{N} \\sum_{z_1} (z_1 - \\bar z_1)^2$ 其中我们假设系数矩阵的模为1 $|w_1| = 1$\n推广到n个实例, $z = Wx$ 求解这样一个正交矩阵$W$\n数学上的求解 最后就是找前$n$大的特征值，组成的一个对角阵就是答案。\n另一种可能的解读方式： 最小化误差的构成\n近似表示$x = \\bar x + c_1 u_1 + \u0026hellip; + c_k u_k$, $[c_1, \u0026hellip;, c_k]$表征一张数字图像; (分解k个成分)\n$x - \\bar x = \\sum_{k=1}^K c_k u_k$\n目标损失：$L = |(x-\\bar x) - \\hat x|^2$最小化\nPCA看起来就是一个单隐层的神经网络, 使用一个线性的激活函数\n","date":"2022 年 12 月 15 日","externalUrl":null,"permalink":"/posts/2022/ml-final/","section":"Posts","summary":"\u003cp\u003eCNN,RNN, Transformer原理; 无监督学习\u003c/p\u003e","title":"期末考试复习(开卷)","type":"posts"},{"content":"下午茶合集，这周的重点感觉就是各种排列问题，以及构造（太难了）\n1205 ARC 100B Equal Cut # https://atcoder.jp/contests/abc102/tasks/arc100_b\n输入$n(4\\le n \\le 1e5)$ 和一个长为 $n$ 的数组 $a (1\\le a[i] \\le 1e9)$\n将 $a$ 分割成 $4$ 个非空连续子数组，计算这 $4$ 个子数组的元素和。 你需要最小化这 4 个和中的最大值与最小值的差，输出这个最小值。\n提示：\n如果只划分2个区域，如果只增加其中一个区域，必然会使得一个区域变得更小，那么就一定让差值变大！所以需要尽可能均匀划分。 4个区域等价于切3刀，那么至少需要维护这三个位置，区间和可以用前缀和来计算。 枚举中间位置，那么剩余两刀总是希望切的尽可能均匀（原因在上面的例子中给出了），那么当我们向右移动中间指针，剩余的两个划分也应该向右（这样保证了我们的算法是$O(N)$的） 1207 ARC 140C # https://atcoder.jp/contests/arc140/tasks/arc140_c\n脑筋急转弯的构造题 上升下降子序列的变形, 构造具有一定的规律 总是希望数字可以尽可能的被用到, 且我们可用的只有排列的$1-N$, 而差值需要严格递增 容易想到构造 $$ X, X-1, X+1, X-2, X+2,\u0026hellip; $$ 这种 或者是 $$ X, X+1, X-1, X+2, X-2,\u0026hellip; $$ 这一种结构\n参考实现（比较简单）\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; #define ll long long #define PII pair\u0026lt;int, int\u0026gt; #define PLL pair\u0026lt;ll, ll\u0026gt; #define VI vector\u0026lt;int\u0026gt; #define VVI vector\u0026lt;VI\u0026gt; #define maxn 1000000007 #define mod 1000000007 int N, X; vector\u0026lt;int\u0026gt; v; int main() { cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; X; for (int i = 1; i \u0026lt;= N; ++i) { if (i != X) { v.push_back(i); } } if (N % 2) { --N; N /= 2; cout \u0026lt;\u0026lt; X \u0026lt;\u0026lt; \u0026#39; \u0026#39;; for (int i = 0; i \u0026lt; N; ++i) { cout \u0026lt;\u0026lt; v[N - 1 - i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; cout \u0026lt;\u0026lt; v[N + i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } else { if (X == N / 2 || X == N / 2 + 1) { // 特判 int sgn = 1, cur = X; if (X == N / 2 + 1) { sgn = -1; } for (int j = 1; j \u0026lt;= N - 1; ++j) { cout \u0026lt;\u0026lt; cur \u0026lt;\u0026lt; \u0026#39; \u0026#39;; cur += sgn * j; sgn = -sgn; } cout \u0026lt;\u0026lt; cur \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } else { cout \u0026lt;\u0026lt; X \u0026lt;\u0026lt; \u0026#39; \u0026#39;; --N; N /= 2; for (int i = 0; i \u0026lt; N; ++i) { cout \u0026lt;\u0026lt; v[N - i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; cout \u0026lt;\u0026lt; v[N + 1 + i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } cout \u0026lt;\u0026lt; v[0] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } return 0; } 1208 ARC 144C K Derangement # 寻找字典序最小的排列，满足条件$|A_i - i| \\geq K, \\forall\\ i(1\\leq i \\leq N)$\n$2 \\leq N \\leq 3 \\times 10^5 $ $1 \\leq K \\leq N - 1$ 题解：\n首先可以发现$K$的大小有限制，$N \\ge 2K$时才有可能有解。\n贪心：如何得到最小字典序的排列？\n一个错误的贪心算法：每一轮操作从满足$|x-i| \\geq K$的最小数字中拿一个。\n当构造排列时，对于每个下标i，需要设置$A_i$;但是由于$A_i$是一个排列，每个数字都只出现一次，需要额外考虑一些条件（即可能出现贪心的选取导致后续再也去不了的情况出现）\n例如：\n8 3 前三个数字，根据要求我们可以取4 5 6 第4个数字满足$|x - 4| \\geq 3$所以$x = 1$或者$x \\geq 7$; 理论上我们可以取1，如果我们确实拿了1，就会变成4 5 6 1 第5个数字只能取 8，第6个数字只能取 3，第7个数字只能取 2，此时我们发现剩下的是7，但是不满足要求，所以第8个数字就没有数字可以取了，说明我们并不能取1.\n一个正确的贪心算法： 对于i = 1,2,3,\u0026hellip; 如果 j = i + K 不在之前的序列中出现过，并且 j \u0026gt; N - K，那么取Ai = j 否则，始终选满足$|x - i|\\geq K$的最小未出现数字，加入序列。 为什么这个算法是正确的？ 分类讨论： 对于$2K \\leq N \\leq 4K$\n对于$2K\\leq N \\leq 3K$ 可以按照这样的方式构造答案(以$N-K$为分界点) 贴一个错误的贪心： $$ A_i = \\begin{cases} i + K,\\ i \\leq N - K \\ i + K - N,\\ N - K + 1\\leq i \\leq N \\ \\end{cases} $$\n正确的贪心结果： 比较复杂的是对于$3K\\lt N \\leq 4K$的情况 这个时候的$N - K$大于$2K$了，不能用上面一样的式子(否则会有重复的值出现)\n$$ A_i = \\begin{cases} i + K,\\ i \\leq K \\ i - K,\\ K \\lt i \\leq N - 2K \\ i + K,\\ N - 2K \\lt i \\leq N - K \\ i + K,\\ N - K \\lt i \\leq 3K \\ i - K,\\ 3K \\lt i \\leq N \\ \\end{cases} $$\n对于$N\\geq 4K$的情况： 我们可以输出$(K+1,\u0026hellip;,2K, 1,\u0026hellip;,K)$作为前$2K$项的结果，剩余的$N-2K$项由上面的两类情况给出答案，从而满足要求。\n难点：实现代码，多种情况\n1209 ARC 132C Almost Sorted # https://atcoder.jp/contests/arc132/tasks/arc132_c\n输入 $n(\\lt 500)$, $d(\\lt 5)$ 和长为 $n$ 的数组 $a$。 $a$ 原本是一个 $1~n$ 的排列 $p$，不过有些数字被替换成了 $-1$。 你需要还原 $p$，使得 $abs(p[i]-i) \\lt d$ 对每个 $i$ 都成立。 输出有多少个这样的 $p$，模 $998244353$。\n题解： 枚举每一位上放什么数字，根据题目限制，实际上可行的数字范围只有2d这么多个，因此用一个2d位长的bit-mask就可以完成状态的表示了。\nQ：是否存在重复表示的问题？ A：不存在，重复使用的数字会体现在之前位置上的bit-mask状态中，因此在后面的位置上，如果可以从前一个状态转移而来一定是合法的。\n参考代码\n","date":"2022 年 12 月 7 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1205/","section":"Posts","summary":"","title":"下午茶合集-1205","type":"posts"},{"content":"","date":"2022 年 12 月 4 日","externalUrl":null,"permalink":"/tags/%E4%B8%8D%E4%BC%9A%E4%B8%8D%E4%BC%9Aqaq/","section":"Tags","summary":"","title":"不会不会qaq","type":"tags"},{"content":"CNN作业记录, 准备试用一下LaTex模板~\nCNN\n截止 12月12日 由 23:59 编辑 得分 0 提交 一份上传文件 Based on the MNIST dataset and Imagenet_mini (optional), design and implement a proper convolutional neural network.\n1. 任务描述 # 采用什么方法实现一个什么样的任务? 解决了什么问题?\n基于MNIST数据集, 设计和实现了一套基于CNN卷积神经网络的分类器系统.(必选)\n基于Mini-ImageNet数据集, 设计CNN卷积神经网络完成分类系统的设计.(可选)\n2. 数据集描述 # 采用何种数据集开展实验？\nMNIST: https://www.kaggle.com/competitions/digit-recognizer Mini-ImageNet, 例如这里kaggle的数据集, 只有1000张图片; https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000/code 3.方法介绍 # 3.1数据预处理(如有) # MNIST数据集的预处理: 没有\n可行的包括去掉空白像素点 Mini-ImageNet\n","date":"2022 年 12 月 4 日","externalUrl":null,"permalink":"/posts/2022/ml-cnn/","section":"Posts","summary":"\u003cp\u003eCNN作业记录, 准备试用一下LaTex模板~\u003c/p\u003e","title":"卷积神经网络总结 CNN","type":"posts"},{"content":"11.30 ARC 136B - Triple Shift # https://atcoder.jp/contests/arc136/tasks/arc136_b\n给定两个数组A和B, 长度为都是$N \\ge 3$\n对于两数组中任意相邻的三个元素$a[i], a[i+1], a[i+2]$, 可以执行以下操作任意次:\n交换$a[i], a[i+1], a[i+2]$, 得到 $a[i+2], a[i], a[i+1]$ 问是否可能让A和B相同?\n解法 # $x,y,z = z,x,y = y,z,x$ 可以发现这样的一个规律(三个元素必然可以排序第一个)\n$N$个元素序列必然可以排序前$N-2$个元素\n于是第一点我们可以直接排序A,B数组, 看构成是否一样?\n构成不一样直接No, 否则继续观察内部元素\n如果存在重复的元素, 那么必然可以把重复元素放到一起(所以肯定可以实现排序)\n如果没有, 就要看最终前N-2排完序后最终是否一致了(因为不能出现x,z,y)这种顺序 (这里可以借助逆序对的奇偶性来反映这一点)\n12.1 ARC 116D - I Wanna Win The Game # https://atcoder.jp/contests/arc116/tasks/arc116_d\n解法 # n个非负整数 所有数的和为m 所有数的异或和为0 $A_1\\oplus A_2​ \\oplus ⋯ \\oplus A_N​=0$ $1 \\le n,m \\ge 5000$ 所以每个元素取值在5000以内\n对于异或和=0问题, 等价于观察每个比特为1出现的次数是否是偶数\n对于相加和等于m, 等价于一个背包问题\n用这样一个数组f[5000][32][2] 表示状态\nf[x][y][z] 表示取得数字和为x, 比特位y上的1出现次数为z的方案数\n每一轮更新 滚动数组, 因为5000 \u0026lt; 8192 所以只会用到前几个比特位(2^13) 所以只有0-13这14个比特位有效\n时间就是O(14NM) = 14 * 25 * 1E6 =\n空间就是O(NM) # 定义 $f[i]$为 n个元素和为i的时候的答案\n推论:\n由于异或和 = 0 意味着 i必须是个偶数(奇数的最后一位必然是1) 考虑$n$个数的二进制最低位, 有$j$个1, 从n个数里面取的有效方案数就是$C(n, j), j = 0,2,4,\u0026hellip;$ 去掉最低位后 其余位数元素和等于$i - j$, 再进行右移, 考虑$f[(i-j)/2]$ 得到了一个递推关系式$f[i] = \\sum_j f[(i-j)/2] \\ctimes C(n,j), \\ j=0,2,4,\u0026hellip;$ 初始状态 $f[0] = 1$\n所求的就是$f[m]$\n12.02 ARC 077B # https://atcoder.jp/contests/abc066/tasks/arc077_b\n计算一下发现只需要关注两个唯一出现的重复位置带来的影响即可 靠左边一侧的重复元素位置记作$x$, 右边一侧的元素记作$y$\n那么对于给定长度$k$, 会产生重复计数的序列数是多少?\n数左边的时候, 会再在右边数到一次 左侧$x - 1$个, 右侧$n - y - 1$个, 从这些里面选$k-1$个得到的序列都是重复的 可以预处理这一段 组合数计算+逆元 可以参考模板\nLC322 将节点分成尽可能多的组 # 被图论暴打了(悲) https://leetcode.cn/problems/divide-nodes-into-the-maximum-number-of-groups/\n贴一个周赛的T4做法\n由于是一个普通图, 我们可以拆分成多个连通分量 下面假设是一个连通图\n考虑图的性质 发现如果是一个树(无环连通图), 那么始终可以满足题目要求 否则, 如果这个图存在奇数环就一定不行; 偶数环的连通图是可以完成分层操作的. 然后所求的答案就是每个连通分量内可以达到的最大分层数量, 这个我们不知道图的形状所以只能通过枚举BFS的起点来求\n如何判断一个图是否含有奇数环(二分图)\n不能二分图的有环图就是有奇数环 偶数环可以对角染色完成二分 class Solution: def magnificentSets(self, n: int, edges: List[List[int]]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; |y - x| = 1 对于同一个点y 从某个节点x到达y 每一跳的组只能差1 所以不同路径的长度差距一定是偶数(路径长度都是奇数/偶数) \u0026#34;\u0026#34;\u0026#34; g = [[] for _ in range(n)] for e in edges: a, b = e[0] - 1, e[1] - 1 g[a].append(b) g[b].append(a) # 时间戳判定BFS顺序 time = [0] * n clock = 0 def bfs(start): dep = 0 nonlocal clock clock += 1 time[start] = clock q = [start] while q: tmp = q q = [] for x in tmp: for y in g[x]: if time[y] != clock: time[y] = clock q.append(y) dep += 1 return dep color = [0] * n def is_bipartite(x, c): color[x] = c nodes.append(x) for y in g[x]: if color[y] == c or (color[y] == 0 and not is_bipartite(y, -c)): return False return True ans = 0 for i, c in enumerate(color): if c != 0: continue nodes = [] # 存在奇数环 if not is_bipartite(i, 1): return -1 ans += max(bfs(x) for x in nodes) return ans ","date":"2022 年 11 月 30 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1128/","section":"Posts","summary":"","title":"灵茶の试炼-1128","type":"posts"},{"content":"","date":"2022 年 11 月 24 日","externalUrl":null,"permalink":"/tags/%E5%80%92%E7%BD%AE%E6%98%9F/","section":"Tags","summary":"","title":"倒置星","type":"tags"},{"content":"","date":"2022 年 11 月 24 日","externalUrl":null,"permalink":"/tags/%E7%BC%BA%E6%B0%A7/","section":"Tags","summary":"","title":"缺氧","type":"tags"},{"content":"倒置星开荒记录\u0026hellip;\n倒置星记录\n炼钢1000kg, 完成制氧模块\n芦苇纤维获取, 气压服锻造(壁虎模块)\n二氧化碳火箭\n原油精炼开发\n储油石+水泉供水产油 =\u0026gt; 精炼石油 =\u0026gt; 石油发电+塑料生产 炼钢模块的制造:\n蒸汽机 * 3 需要600kg的塑料, 因此大概需要准备1000kg的石油 钢制液冷机 1200kg钢 导热管+冷却液(石油) 能源(电力的解决方式)\n煤炭 =\u0026gt; 石油+天然气 600W =\u0026gt; 2000W 二星开发 太阳能 化石, 炼钢, 养哈奇 成就系统介绍\n几个比较麻烦的\n发射火箭到18w千米 建造丰碑 土食成就 肉食成就 可持续发展(不能用) 集齐动物变种(拖时间) 海牛(纯拖时间用的) 招20个小人 准备招满直接送走 所有小人穿气压服连续完成10周期任务 其余没有时效性的没有什么难度, 就是会拖游戏进度, 比较麻烦; 正好也可以趁这段时间修一些复杂的开发模块, 比如开发火山或者是高温蒸汽喷孔, 改进一些养殖模块(加入自动补充, 手动补充对于寿命长的动物也完全合理, 因为很久都不用操作w)\n冰霜星球全成就, 其实这个存档的模块大致都是完整的, 因为需要冲击100周期40w肉食的成就, 直接通过上天抓田鼠的方式完成, 等于是100个周期里面必须上太空(距离就很长), 加上别的成就拖进度就会很麻烦, 一般游玩根本不需要考虑能源的问题, 但是你为了做成就不能用化石能源(煤炭, 天然气, 石油)\n","date":"2022 年 11 月 24 日","externalUrl":null,"permalink":"/posts/2022/oni/","section":"Posts","summary":"","title":"缺氧游戏记录","type":"posts"},{"content":"令人吐血的作业\u0026hellip;\n机器学习作业-欺诈识别报告 # 1. 任务描述 # 2. 数据集描述 # 数据集包括了三个部分:\ndriver_data，建模样本表，每行数据是一笔交易，数据可以分为几部分：交易主键，交易主体， 交易属性，数值型变量，行为序列编码，是否欺诈的标签；\nevent_data_card， driver数据中收益卡近15天(从driver表中的交易时间算起)的历史收款数据，包含交易主键，交易主体，交易属性；\nevent_data_user，driver数据中用户近15天的历史付款数据，包含交易主键，交易主体， 交易属性(注意这里的付款场景不止转账到卡的交易数据);\n详细说明 # 建模样本表driver_data的数据描述\n交易属性\n交易时间\n交易的用户(用户id)\n收款方id(b_id, c_id)等\n交易用户信息(基本属性)\n收款方信息(收款为账号) b_info_1\n收款方信息(收款为卡) c_info_1\n用户信息 a_info_1, a_info_2, a_info_3\n交易的设备位置信息 e_info2, e_info3\n用户信息\na_id_1 + a_info_1-7\na_info_4, a_info_6 这两列明显种类数量较少 适合使用Onehot编码\n其余的a_info3,5,7等的量级在70-350之间 可以正常使用label-encoding\na_id_1这个用户标识几乎没有重复的, 让人怀疑究竟是怎会做到的, 即使存在诈骗行为也是很多的人(切换不同的账号进行诈骗)\n3.方法介绍 # 数据处理的关键点:\n数据分散在多张数据表里面(包括了交易模型, 卡片的交易记录, 用户的交易记录等), 需要用合适的方式组织数据;\n数据类型复杂, 包括了大量的STRING类型数据, 序列化数据等无法被模型使用的类型, 需要进行转换处理;\n数据的特征不明显, 由于很多的数据都被经过编码, 无法从编码本身看出对应的含义;\n因此, 结合以上几个问题, 我们首先对于三张数据表进行了特征的筛选和合并, 得到相应的用于训练和测试的数据集\n对于数据特征工程: 提取重要的特征, 重新组成一些新特征\u0026amp;去掉一些特别离谱的数据\n模型选择：对于此类识别的问题来说, 基于决策树/随机森林的模型一般不错\n正负类差距过大, 可以通过一些方式, 例如imblearn进行适当修正, 或者使用负采样的方式 提升负类比例 down-sampling\n3.1数据预处理(如有) # 数据类型复杂, 含义较为模糊, 在这里进行了比较多的预处理工作\ndriver_data表的处理 # 总结一下, 给我们的driver_data数据表中的数据主要就是以下这些\n发起交易的用户信息\n收款方信息, 这里包含了两种收款方\n收款方为卡 收款方为体系内账号 交易双方的设备位置信息\n交易发生的时间(时间+日期+消失)\n交易金额\n交易是否完成\n那么从中我们发现, driver_data里可以提取的特征例如:\n观察收款方的特点, 是否与诈骗率有关系? 收款为卡和收款为账号 GPS位置信息与诈骗率关系\n组合以上特征得到的一些新的, 更高级特征是否与诈骗率有关系?\nevent_card表的处理 # event_user表的处理 # 由于交易行为表中关于用户的交易记录完全缺失, 所以在我们的模型训练中没有包含这一张表的信息.\n3.2算法描述 # 诈骗识别采用的模型是随机森林+参数优化算法\n4.实验结果分析 # 4.1评价指标 # 4.2定量评价结果 # 4.3可视化结果 # 5.总结 # 比较不同方法的优缺点等。 特征工程 # 做特征的思路\n因为是识别当前交易是否为欺诈交易，在做特征之前可能需要基于一定的业务背景分析下欺诈的场景，一般来讲，线上支付欺诈一般是盗号，那么实施欺诈时可能会有两种表现\n盗取后，集中高频交易，从而在最短时间内套现;\n模拟真实用户的交易特征，在尽可能不被系统和用户发现的前提下实现盗刷等;\n提取特征 # 去除了7列无关紧要的数据列(这里主要是用户信息, 因为无论是在训练集和还是测试集都没用!)\n去掉了训练集中所有被系统拦截的交易(if_succ = -1)这些全部去掉 总共900+行数据.\n在XGBoost模型下反而降低了ROC-AUC到82%(性能变差了! 说明好像去掉了什么重要的内容!)\n可能的改进方案:\n时间序列数据 在训练+测试集上是有一个明显的先后顺序的! todo:\ndt 一项实际上和time有重复! (time包含了当天的小时+分钟+秒等信息) apply自定义时间(YY MM DD: hh:mm:ss to 时间戳整数)\n将time转为时间戳, 去掉dt(重复了), 序列化week_day(0-1-2-3-4-5-6)表示星期天-星期六即可, hh小时可以不用改\n增加特征, 替换time(绝对时间) time_in_day 表示天内的时间偏移量\ncents 表示 交易金额取整之后剩余部分()\ndf[\u0026rsquo;time_in_day\u0026rsquo;] = uni.TransactionDT % 86400 df[\u0026lsquo;cents\u0026rsquo;] = uni.TransactionAmt % 1\n使用lgb替代原先使用的XGBoost(实际上可以两种都使用)\n借鉴他人的方案\n我认为这个或许可以替代手工实现一波特征的生成(引用openfe的feature-generation方法)\nhttps://github.com/IIIS-Li-Group/OpenFE/blob/master/examples/IEEE-CIS-Fraud-Detection/main.py\n或者这个比较传统的方式\nhttps://www.kaggle.com/code/jtrotman/ieee-fraud-adversarial-lgb-split-points\nCached OOF(Out-Of-Fold) 折外预测 FAST Cross 特征分析 # 用户信息\na_id_1 + a_info_1-7\na_info_4, a_info_6 这两列明显种类数量较少 适合使用Onehot编码\n其余的a_info3,5,7等的量级在70-350之间 可以正常使用label-encoding\na_id_1这个用户标识几乎没有重复的, 让人怀疑究竟是怎会做到的, 即使存在诈骗行为也是很多的人(切换不同的账号进行诈骗)\n发起交易方的用户信息数据列 # 用户信息 主要是以下这些列相关的信息\n[\u0026lsquo;a_id_1\u0026rsquo;, \u0026lsquo;a_info_1\u0026rsquo;, \u0026lsquo;a_info_2\u0026rsquo;, \u0026lsquo;a_info_3\u0026rsquo;, \u0026lsquo;a_info_4\u0026rsquo;, \u0026lsquo;a_info_5\u0026rsquo;, \u0026lsquo;a_info_6\u0026rsquo;, \u0026lsquo;a_info_7\u0026rsquo;] a_id_1基本上都不相同(基本上都是不同的用户) a_info_[1-7] 这些给出的都是uuid编码数据 看得出来比较分散, 需要LABEL-ENCODING后进行处理\n地理位置相关的参数 # [\u0026rsquo;e_info_2\u0026rsquo;,\u0026rsquo;e_info_3\u0026rsquo;,\u0026rsquo;e_info_4\u0026rsquo;,\u0026rsquo;e_info_5\u0026rsquo;] 4个编码字符串 类别化之后是否可以发现一些关系?\n注意是否需要和测试集一起编码?\n收款方相关的参数 # 收款方是用户\n[\u0026lsquo;b_id_1\u0026rsquo;,\u0026lsquo;b_info_1\u0026rsquo;, \u0026lsquo;b_info_2\u0026rsquo;, \u0026lsquo;b_info_3\u0026rsquo;, \u0026lsquo;b_info_4\u0026rsquo;, \u0026lsquo;b_info_5\u0026rsquo;] 总结: 根本就没有, 可以忽略\n收款方是 卡\n[\u0026lsquo;c_id_1\u0026rsquo;, \u0026lsquo;c_info_1\u0026rsquo;, \u0026lsquo;c_info_2\u0026rsquo;, \u0026lsquo;c_id_2\u0026rsquo;, \u0026lsquo;c_info_4\u0026rsquo;, \u0026lsquo;c_info_5\u0026rsquo;] 很多的编码字符串 类别化之后是否可以发现一些关系?\n注意是否需要和测试集一起编码?\n我认为有必要先划分driver data成两半\n收款是用户的 merge 用户\n收款是卡的 merge 卡\n然后分别在对应的主体上生成模型, 最后进行评估\nc_id_1 不清楚 23个相同用户 保留\nc_id_2 不清楚 911个共同特征 保留\nc_info_1 也可以留着(虽然关联不大 272个共同标签\nc_info_2 可以保留\nc_info_3 这个好多空值 建议去掉\nc_info_4 可以保留\nc_info_5 可以保留\n交易相关参数 # 这些:\n[\u0026rsquo;time\u0026rsquo;, \u0026lsquo;hh\u0026rsquo;, \u0026lsquo;week_day\u0026rsquo;,\u0026rsquo;e_info_6\u0026rsquo;,\u0026rsquo;e_info_13\u0026rsquo;,\u0026lsquo;if_succ\u0026rsquo;, \u0026lsquo;dt\u0026rsquo;]\ne_info_6 反映了交易类型(完全一致)\ne_info_13 标识了交易金额相关\nif_succ = 0的案例可以丢到(防止影响)\ntime其实没什么用(看起来) 测试集合的时间都在训练集合后面\ne_info_6这一列数据在测试集合和训练集合一模一样 去掉\nhh和week_day还是比较有用的\ne_info_13分布相近 可以利用\ndt同理 没有什么用\n总结 # 这一些参数内部 可以被作为\u0026rsquo;类别category\u0026rsquo;的列有以下:\n而剩余的non-numeric列自然成为了\u0026rsquo;ordinal_features\u0026rsquo;\n通过open_fe的特征生成, 使得最终成绩提高到了0.34, 说明很有效果(特征的选择)\n继续提升的话需要考虑更精细的特征组合(高维度的特征加入) 模型继续使用随机森林\n比如组合\u0026quot;地理信息\u0026quot;, \u0026ldquo;用户信息\u0026quot;为新标签列加入数据集 加入一些_FREQUENCY列(例如某一个标签出现的频率) 细分AMOUNT(包括零头部分) ","date":"2022 年 11 月 24 日","externalUrl":null,"permalink":"/posts/2022/ml-fraud/","section":"Posts","summary":"\u003cp\u003e令人吐血的作业\u0026hellip;\u003c/p\u003e","title":"机器学习作业-欺诈识别","type":"posts"},{"content":"下午茶知识点: 二维前缀和, 贪心\u0026amp;构造, 双指针\u0026amp;找规律\n11.21 106D AtCoder Express 2 # https://atcoder.jp/contests/abc106/tasks/abc106_d\n11.22 178F Contrast # https://atcoder.jp/contests/abc178/tasks/abc178_f\n找众数, A+B数组中出现超过了N次 =\u0026gt; 不可能实现, No;\n否则一定可以!(具体怎么构造?)\n输入按照升序排列, 构造的时候刻意逆序(这样可以尽可能避免相同)\n如果还是相同了! 从前往后遍历, 就从前面换(前面的至少大于等于)\n由于之前的保证, 我们肯定有解, 所以不会死循环\n11.23 C - Swaps 2 # https://atcoder.jp/contests/arc120/tasks/arc120_c\na和b最终如果可以经过操作相等\na[1], \u0026hellip;, a[n]\n左移x次 变成 a[i] + X 右移X次 变成 a[i] - X\n如果最终 a[i]和b[j]匹配, 那么 $a[i] + p - q = b[j]$ 成立\n$a[i] + i == b[j] + j$成立(不太清楚怎么能够推出来的\u0026hellip;)\n通过相邻交换数组a\u0026rsquo; 变成b\u0026rsquo;所需要的最小操作次数(等价于求逆序对数量)\n记录a数组内所有元素及其对应位置(对于相同元素: 从小到大)\n遍历b数组, 观察当前b数组的值v在a中出现的位置\n如果不存在 说明不可能实现 返回-1 否则, 从a中对应元素的位置中取出第一个位置j, 当前位置为i 那么邻项交换这一段的代价就是[j, i]这一段的逆序对数量 使用树状数组维护逆序对数量.. 11.24 133C - Row Column Sums # 构造题\n给定a, b数组, 构造一个矩阵满足\n第i行的和 % K = a[i] 第j列的和 % K = b[j] 矩阵元素取值范围为 $[0, K - 1]$\n如果可以做到, 给出矩阵的最小元素和, 否则输出-1\n初始化这个矩阵如下: 全部填最大, 然后逐渐缩小\n$$ \\begin{bmatrix} k-1 \u0026amp; k-1\\ k-1 \u0026amp; k-1 \\end{bmatrix} $$\n对于第$i$行的元素, 需要减小$((k-1)\\times m - a[i]) % k$ 对于第$j$列的元素, 需要减小$((k-1)\\times n - b[j]) % k$\n证明至少要减小$Z=max(\\sum_i C_i, \\sum_j D_j)$, 且不会出现负数.\n$(\\sum_{1\\leq i \\leq H} C_i) \\ mod\\ K = (\\sum_{1\\leq j \\leq W} D_j) \\ mod\\ K$\n11.25 119B - Electric Board # https://atcoder.jp/contests/arc119/tasks/arc119_b\n题中给到的交换操作只限于\n交换0(1)+的 最左侧\u0026amp;最右侧字符 变成 11111\u0026hellip;0的形式 交换1(0)+的 最左侧\u0026amp;最右侧字符 变成 10000\u0026hellip;0的形式 重要性质:\n不会对0的数量产生影响 对于已经在对应位置上的0 我们不需要交换 否则说明需要 假设S中的0位置为 i1, T[i1] = 1说明需要交换\n交换给谁? 证明一下发现按照相对位置顺序交换是最优的, 为什么?\nS中相邻的两个0, 位置为$i_1, i_2$, 且满足$i_1 \u0026lt; i_2$ T中相邻的两个0, 位置为$j_1, j_2$, 且满足$j_1 \u0026lt; j_2$ 如果不对应顺序 =\u0026gt; 交换必然导致 $i_1\\to j_2$, $i_2 \\to j_1$ 这种情况发生\n而 $i_1 \\to j_1$, $i_2 \\to j_2$ 的代价更小(没有重叠段) 所以只需要按照顺序依次求出0位置的差值即可.\n","date":"2022 年 11 月 22 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1121/","section":"Posts","summary":"\u003cp\u003e下午茶知识点: 二维前缀和, 贪心\u0026amp;构造, 双指针\u0026amp;找规律\u003c/p\u003e","title":"灵茶の试炼-1121","type":"posts"},{"content":"简介：下午茶合集11.14-11.19，可能会咕咕咕 知识点: 最小生成树, 数论, 二维矩阵二分, 构造\n11.14 ABC # 11.15 ABC 210E # https://atcoder.jp/contests/abc210/tasks/abc210_e\n题目简介:\n输入 $n(\\leq 1e9)$ 和 $m(\\leq 1e5)$，表示一个 $n$ 个点，$0$ 条边的图（节点编号从 $0$ 开始），以及 $m$ 个操作。 每个操作输入两个数 $a(1\\leq a \\leq n-1)$ 和 $c(≤1e9)$，表示你可以花费 $c$，任意选择一个 $[0,n-1]$ 内的数字 x，把 x 和 $(x+a)%n$ 连边。 这 $m$ 个操作，每个都可以执行任意多次，可以按任意顺序执行。 输出让图连通需要的最小花费。 如果无法做到，输出 $-1$。\n题解 # 实际上就是模拟Kruskal算法寻找最小生成树的过程，但是考虑到这里这个图的边数达到了$NM$，显然不能直接使用MST算法直接求\n按照cost的大小升序排序，不停的从排序后的 操作1，操作2，\u0026hellip;开始操作\n精髓在于怎么找到 \u0026ldquo;完成某些操作之后\u0026rdquo;, 连通分量的变化(这里可以发现 连通分量的数量实际上就是 约数的大小)\n所以我们贪心的选取最小的代价边不断操作直到无法减少分量为止, 而这个操作次数怎么求?(实际上操作次数 == 连通分支的缩小量); 而连通分量的变化量推理后发现就是$gcd(N, A_1, \u0026hellip;, A_n)$, 于是我们就找到了答案$\\sum_{i} C_i(X_{i-1} - X_i)$, $X_i$表示用完第$i$个操作后连通分量的数量\nhttps://atcoder.jp/contests/abc210/editorial/2307\n11.16 ABC 203D # https://atcoder.jp/contests/abc203/tasks/abc203_d\n题目简介: 对于一个$N\\times N$的矩阵, 给定大小$K$, 找到所有$K\\times K$的子矩阵中的中位数中最小的元素(即所有子矩阵中位数的最小值)\n题解 # 二分答案(子矩阵最小值) $logU$, 观察是否可以取到;\n对于一个矩形区域, 如何快速得知 小于等于某一个数$x$的元素个数?\n二维区域的前缀和($N^2$的复杂度进行维护)\n算法总复杂度$N^2logU,\\ U = 1e9$\n","date":"2022 年 11 月 13 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1114/","section":"Posts","summary":"","title":"灵茶の试炼-1114","type":"posts"},{"content":"","date":"2022 年 11 月 13 日","externalUrl":null,"permalink":"/tags/%E4%B8%8B%E5%8D%88%E8%8C%B6/","section":"Tags","summary":"","title":"下午茶","type":"tags"},{"content":" MLPNN 实验报告 # 1. 任务描述 # 采用什么方法实现一个什么样的任务? 解决了什么问题?\n基于MNIST数据集, 设计和实现了一套分类器, 基于多层感知机MLP模型，并且尝试使用多种分类的损失函数进行测试。\nBased on the MNIST dataset, design and implement MLPNN with two different error functions for classification.\n2. 数据集描述 # 采用何种数据集开展实验？\nMNIST数据集(kaggle地址)\nhttps://www.kaggle.com/competitions/digit-recognizer\n基于了手写数字-MNIST数据集 数据集有什么特点？\nMNIST-手写数字数据集 每一张图片都是28 * 28大小的标准图片 数据集合的大小比较大, 有42000张手写数字的图片; 测试集合也有28000张 数据集合中每种数字的分布比较的均匀 数据集的各项统计量是多少？\n数据集大小：42000张28*28的数字灰度图片； 数据集参数数量：每一张图片的大小都是28*28，数字范围0-9，总计42000个输入训练样本。 训练样本中数字的具体的分布如下： {% asset_img digits.png This is an example image %}\n3.方法介绍 # 3.1数据预处理(如有) # MNIST数据集 由于输入的没有任何预处理, 会提前观察一下图片的情况\n例如这里随机打印了25个数字的图片\n可以观察到手写数字的大致形状和图片的基本属性\n3.2算法描述 # 使用的模型是多层感知机MLP\nMLP的结构:\n输入层 隐层 输出层 特点:\n输入层大小为输入向量的维数\n隐层之间的神经元是全连接的\n隐层神经元的激活函数是非线性的\n对于网络中的第j个神经元而言, 它所收到的响应为 $net_j = \\sum_{i = 1}^d w_{ji} x_i + w_{j0} = \\sum_{i = 0}^d w_{ji} x_i = w_j^T \\cdot x$\n因此, 第j个神经元的输出就是$y_j = g(net_j)$\n对于输出层的第$k$个神经元, 它的信号相应就是$net_k = \\sum_{j = 1}^{n_H} w_{kj} y_j + w_{k0} = \\sum_{j=0}^{n_H}w_{kj} y_j = w^T_k \\cdot y$\n最终对于标签分类为$k$的输出就是$y_k = f(net_k) = f(w^T_k \\cdot y) = f(\\sum_{j = 0}^{n_H} w_{kj} f(net_j)) = f(\\sum_{j = 0}^{nH} w_{kj} f(\\sum_{i = 0}^d w_{ji} x_i))$\n对于二分类问题而言, $y_k \\in {1, -1}$\n对于多分类问题, $y_k = f(net_k) = g_k(x)$\nMLP采用的不同Loss function\nSSE Sum of Squared Loss 物理意义上, 就是实际输出与我们预测的输出之间的方差\n$$ E(w) = \\frac{1}{2} \\sum_{n = 1}^N (y(w, x_n) - t_n)^2 $$\n概率上的解释: 网络输出给出了一种概率上的分布, 使用概率的好处 1. 可以引入非线性 2. 更多的loss选择 从概率论的角度分析\nNN学习到的是一个关于输出变量$t$的概率分布\n$P(t|w, x) = N(t| y(w,x), \\beta^{-1})$\n学习到的是一个高斯分布, 期望$\\mu = y(w,x)$ 方差为$\\beta^{-1}$ 假设我们的输入是独立同分布的, 那么对于所有的标签$t=(t_n)_{n=1}^N$ 我们列出似然函数\n$P(t|w,x,\\beta) = \\Pi_{n=1}^n P(t_n|w,x_n, \\beta)$\n$P(t_n | w, x_n, \\beta) = \\frac{\\beta}{\\sqrt 2\\pi} exp(\\frac{-\\beta}{2}(t_n - y(w,x_n)^2))$\n对数似然函数就是 $L = \\sum_{n=1}^N ln(P(t_n | w,x_n,\\beta)) =\\frac{N}{2}(ln\\beta - ln 2\\pi) - \\frac{\\beta}{2}(y(w,x_n) - t_n)^2 $\n让对数似然函数最大 等价于 让 $\\frac{\\beta}{2}(y(w,x_n) - t_n)^2$ 最小化,正好就是平方误差SSE最小化!\n因此从概率角度我们发现概率最大 == 误差最小, 可以转变为一个学习概率分布的问题\n在本次实验中,我们尝试使用的误差函数有:\nCross-Entrophy Loss 交叉熵 对于一个二分类问题而言\n$t$是输出变量;\n$t = 1$对于$C1$; $t = 0$对于$C2$; 网络有单个输出, 输出激活函数是\n$$ y = \\sigma(a) = \\frac{1}{1 + e^{-a}} $$\n那么输出为$t$的概率是: $$ P(t|X, w) = y(x, W)^t (1 - y(x, W))^{1 - t} $$\n相应的误差函数\n$$ E(w) = -\\sum_{n = 1}^N{t_n ln y_n + (1 - t_n) ln(1 - y_n) } $$\nK分类 输出激活函数为 logistic函数\n输出标签值为$t$的概率就是 $$ P(t|w,x) = \\Pi_{k=1}^K y_k(w,x)^{t_k} (1 - y_k(w, x))^{1 - t_k} $$ 对应的损失函数 $$ E(w) = -\\sum_{n=1}^N\\sum_{k=1}^K{t_{nk} ln y_{nk} + (1 - t_{nk}) ln (1 - y_{nk})} $$\n1-of-K coding\n不断地用二分类的方式区分标签值为k的概率与非k的概率 输出$y_k$的概率公式: $y_k(w,x) = \\frac{exp(a_k(w,x))}{\\sum_j exp(a_j(w,x))}$ MultiLabel Soft-Margin Loss Multi Labels, 1 vs all, max entropy, (x, y) in (N, C) 多标签, 1对全部的损失函数, 基于的是最大熵模型\n具体的计算是 $loss(x, y) = \\frac{1}{C}\\sum_i y_i * log(\\frac{1}{(1 + e^{-x_i}}) + (1 - y_i) * log(\\frac{e^{-x_i}}{1 + e^{-x_i}}) $\n多层感知机的BP算法\nError BackPropagation\n从隐藏层到输出层的学习\n由于输出层 第$k$个单元的响应为\n$net_k = \\sum_{j=0}^{n_H} w_{kj} y_j$\n所以对于给定的$E(w)$, 关于隐层到输出层系数的梯度\n$$ \\frac{\\partial E}{\\partial w_{kj}} = \\frac{\\partial E}{\\partial net_k} \\frac{\\partial net_k}{\\partial w_{kj}} = -\\delta_{k} \\frac{\\partial net_k}{\\partial w_{kj}} $$\n$\\delta_k$是第$k$个单元的sensitivity(敏感度) 假设激活函数f是可微分的\n$$ \\delta_k = \\frac{\\partial E}{\\partial net_k} = \\frac{\\partial E}{\\partial y_k} \\frac{\\partial y_k}{\\partial net_k} =(t_k - y_k) f\u0026rsquo;(net_k) $$\n(因为这里的误差函数就是SSE)\n又因为 $net_k = w_{kj}^T \\cdot y_j$ 所以$\\frac{\\partial net_k}{\\partial w_{kj}} = y_j$\n最终得到隐层-输出层的梯度学习规则\n$$ \\Delta_{kj} = \\eta \\delta_k y_j = \\eta (t_k - y_k) f\u0026rsquo;(net_k) y_j $$\n其中$\\eta$是我们定义的学习率\n从输入层到隐藏层的学习 第$j$个神经元的响应$net_j = \\sum_{i=0}^d w_{ji}x_i$\n而根据我们的结构, 可以列出以下的梯度关系 $$ \\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial net_j} \\frac{\\partial net_j}{\\partial w_{ji}} $$\n由于 $\\frac{\\partial net_j}{\\partial w_{ji}} = x_i$ 以及 $\\frac{\\partial y_j}{\\partial net_j} = f\u0026rsquo;(net_j)$\n还有误差函数关于输出y的导数 $$ \\frac{\\partial E}{\\partial y_j} = \\frac{\\partial}{\\partial y_j}[\\frac{1}{2}\\sum_{k=1}^c (t_k - y_k)^2] = -\\sum_{k=1}^c(t_k-y_k) \\frac{\\partial y_k}{y_j} = -\\sum_{k=1}^c(t_k-y_k) \\frac{\\partial y_k}{net_k} \\frac{\\partial net_k}{y_j} = -\\sum_{k=1}^c(t_k-y_k) f\u0026rsquo;(net_k) w_{kj} $$\n类似的我们定义一个隐层单元的敏感度 $\\delta_j = f\u0026rsquo;(net_j) \\sum_{k=1}^c w_{kj} \\delta_k$\n那么对于输入层-隐层的学习规则我们得到\n$$ \\Delta w_{ji} = \\eta \\delta_j x_i = \\eta f\u0026rsquo;(net_j) [\\sum_{k=1}^c w_{kj}\\delta_k] x_i $$\nMLP在实际应用中需要考虑的问题\n激活函数的选择: 非线性/线性, 单调性, 连续性\n隐层的神经元数量 $n_H \\to n/10$\n初始网络的权重\n学习率大小$\\eta = 0.1$\n权重的衰减\n隐层的数量\n目标函数的选择\n4.实验结果分析 # 4.1评价指标 # 对于MNIST的数据集的数字分类问题，我们使用的评价指标自然是分类的准确率 $p$ (分类正确个数/总测试样本数)\n这里既包括了 训练的准确率 $p_{train}$ 也包括了最终 测试的准确率 $p_{test}$(通过kaggle-submission获得结果) 4.2定量评价结果 # MNIST识别数字的准确率\n实验结果:\n多层感知机+不同的损失函数得到的准确率:\n交叉熵损失: 准确率 $p = 0.97475$\nNLL(对数似然损失) negative log likelihood loss: 准确率 $p = 0.97475$\n4.3可视化结果 # 可以是算法的实际分类效果图(见上4.2部分)\n1.交叉熵损失 # (使用sklearn的版本)\n关于MNIST数据集, 采用基本的MLPNN训练(MLPClassifier, 100个神经元), 学习到的权重分布图(使用sklearn)\n训练的代码:\n可以看到多层感知机模型是如何通过学习权重实现具体的分类, 例如第3行的第3张图片, 可以很明显看出在'1\u0026rsquo;数字附近的权重大小有所不同(说明MLP学习到了边界的分布)\n最终得分\n准确率为$p_1 = 0.950638$, 达到了一个不错的水平 可以看到的是我们的感知机隐层数量只有40个神经元，但是也能够很好的学到特征 参数设置为: 隐层大小 $n_H = 40$ 学习率 $\\eta = 0.2$ 梯度下降使用 sgd 随机梯度下降 损失函数 系数$\\alpha = 0.0001$ (使用torch的版本)\ntorch的相关网络模型定义:\ntorch的训练代码:\ndef train(model, loss_fn, optimizer, train_loader): mean_train_losses = [] mean_valid_losses = [] valid_acc_list = [] epochs = 30 for epoch in range(epochs): model.train() train_losses = [] valid_losses = [] for i, (images, labels) in enumerate(train_loader): optimizer.zero_grad() outputs = model(images) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() train_losses.append(loss.item()) if (i * 128) % (128 * 100) == 0: print(f\u0026#39;{i * 128} / 50000\u0026#39;) model.eval() correct = 0 total = 0 with torch.no_grad(): for i, (images, labels) in enumerate(valid_loader): outputs = model(images) loss = loss_fn(outputs, labels) valid_losses.append(loss.item()) _, predicted = torch.max(outputs.data, 1) correct += (predicted == labels).sum().item() total += labels.size(0) mean_train_losses.append(np.mean(train_losses)) mean_valid_losses.append(np.mean(valid_losses)) accuracy = 100*correct/total valid_acc_list.append(accuracy) print(\u0026#39;epoch : {}, train loss : {:.4f}, valid loss : {:.4f}, valid acc : {:.2f}%\u0026#39;\\ .format(epoch+1, np.mean(train_losses), np.mean(valid_losses), accuracy)) return mean_train_losses, mean_valid_losses, valid_acc_list MNIST使用交叉熵(Cross-Entropy)作为损失函数时的训练准确率曲线(使用torch)\ntorch的测试:\n可以看到很快网络就收敛, 损失很快收敛到一个较低的水平。\n具体的测试过程中 我们可以看到一个步长(epoch)就使得我们的感知机在训练集上几乎学到了所有特征, 获得了90%以上的准确率\n训练集上的准确率$p_{train} = 0.9686$ 最终结果\n我们的多层感知机(使用交叉熵损失)在测试集上的准确率$p = 0.97475$ 达到了一个很高的水平 2.MultiLabel SoftMargin Loss # 多标签-软间隔损失的定义 可以看到实际上类似于交叉熵, 但这里的损失函数, 基于的是最大熵模型\n1 vs k的想法的一个具体应用\n相应的代码:\nnew_model = MLP() loss_fn = nn.MultiLabelSoftMarginLoss() optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001) ## 使用 MultiLabelSoftMarginLoss 损失的测试 mean_train_losses, mean_valid_losses, valid_acc_list = modified_train(new_model, loss_fn, optimizer, train_loader) 训练的过程:\n训练的最终准确率$p_{train} = 0.9762$ 最终结果\n我们的多层感知机(使用多标签软间隔损失)在测试集上的准确率$p_{test} = 0.97032$ 也达到了一个很高的水平 5.总结 # 比较不同方法的优缺点等。 多层感知机模型在数字分类问题上的表现都很不错，即使是最简单的单隐层模型；MLP的一个优点就是十分灵活，可以简单也可以复杂。\n实验中我们测试了不同隐层大小对于分类结果的影响，发现当大小较小时相对而言效果总是稍微差于更大的网络，但是差距并不是很大。\n对于不同的损失函数而言，可以发现不同的loss-function，即使都是基于最大似然估计的概率分布，目标都是去最大化这个似然函数，训练的效果也会略有不同。而如何选择一个合适的loss-function对于我们的模型效果也是很重要的。\n","date":"2022 年 11 月 6 日","externalUrl":null,"permalink":"/posts/2022/mlp-nn/","section":"Posts","summary":"","title":"mlp-nn","type":"posts"},{"content":"","date":"2022 年 11 月 6 日","externalUrl":null,"permalink":"/tags/svm/","section":"Tags","summary":"","title":"SVM","type":"tags"},{"content":" SVM/SVR-机器学习实验报告 # 1. 任务描述 # 采用什么方法实现一个什么样的任务? 解决了什么问题?\n基于MNIST数据集, 设计和实现了一套分类器系统, 包括线性SVM和非线性SVM. Based on the MNIST dataset, design and implement classifiers including: linear \u0026amp; nonlinear SVM.\n设计一套回归系统, 包括SVR(支持向量回归)的算法. Design a regression system to predict housing prices. The data are available at: https://www.kaggle.com/vikrishnan/boston-house-prices (also available at Sklearn). The regression algorithms should contain SVR.\n2. 数据集描述 # 采用何种数据集开展实验？\nMNIST: https://www.kaggle.com/competitions/digit-recognizer\nBoston Prices: https://www.kaggle.com/vikrishnan/boston-house-prices\n第一个实验是基于了手写数字-MNIST数据集\n第二个实验则是基于波士顿房价数据集\n数据集有什么特点？\nMNIST-手写数字数据集 每一张图片都是28 * 28大小的标准图片 数据集合的大小比较大, 有42000张手写数字的图片; 测试集合也有28000张 数据集合中每种数字的分布比较的均匀 数据集的各项统计量是多少？\n数据集大小：42000张28*28的数字灰度图片； 数据集参数数量：每一张图片的大小都是28*28，数字范围0-9，总计42000个输入训练样本。 训练样本中数字的具体的分布如下： Boston Standard Metropolitan Statistical Area (SMSA) in 1970. 数据标签的维度很多（包括了是否沿河？教育资源水平，税收，交通，房屋的新旧等等各种可能的影响因素）\n数据量小（总量只有506行），而且可能数据的年代比较久远了\n数据的特点：以下是数据集的热力图，可以看出有一些列之间的相关度很高，比如TAX和RAD这两个指标几乎就是完全相关的（所以后续也有消除这些相关重复的尝试）\n数据集的各项统计量是多少？\n数据集大小：506行数据 数据集参数数量：11个输入参数，1个输出参数（价格中位数） CRIM（犯罪率），CHAS（是否沿河），AGE（房屋年龄）等等\u0026hellip;\u0026hellip; 数据集各项的单位都不一致，有bool类型，有实数类型，有百分比，千等等不同的单位 3.方法介绍 # 3.1数据预处理(如有) # 第一个Part的MNIST数据集 没有任何预处理, 会提前观察一下图片的情况\nHouse Price数据集 没有对数据项的直接处理，因为数据集没有缺少的数据项，数据值的分布范围看起来都比较正常，合理；虽然单位不一致，但是经过后续的转化后，都可以化成标准分布的数据，从而不会产生很大的影响。\n3.2算法描述 # 两个实验所用到的核心都是SVM算法, 这里一并介绍:\nSVM-支持向量机\n输入: 样本集合 $T = {(x_1, y_1), \u0026hellip;, (x_n, y_n)}$, $x$为输入变量, $y$为输出值(eg. 属于的类别)\n输出: 我们的SVM模型 $y = w^Tx + b$\n$w^T = w^{\\star}$, $b = b^{\\star}$\n满足条件 $maxmin(f(x_i)) = max(min|{w^{*}}^{T}x + b|)$\n其中 我们记录$\\rho = |w^{T}x + b|$ 为样本点的绝对距离, 由于此时我们的系数$w^*$的模大小是1, 因此实际的距离$y_i *f(w^Tx+b)/|w|$也就是上式的值(由于我们是一个二分类问题, $y_i$取值为${1, -1}$)\n映射函数: $y = f(x_i) = w^Tx + b$\n目标: 最大化 样本点到 决策平面的最小距离, 此时落在这个最小距离上的样本被称为支持向量\n目标函数 $min_w{ \\frac{1}{2} |w|^2},\\ s.t.\\ \\forall_{i = 1}^n y_i(w^Tx_i + b) \\geq 1$\n要证明我们的分类器SVM能够收敛, 我们这里沿用感知机模型的结论(感知机不要求这个函数距离的最小, 只要求分类正确因此可以存在很多个)\n收敛性, 最多经过$(b^2 + 1)(R^2 + 1) / \\rho^2$ 次更新,就能找到答案($R = max_i ||x_i||$)\nSVM的重要参数: $\\rho$ 几何距离大小(当$|w| = 1$时也就是函数距离)\n$\\rho$(margin) 决定了:\n两个类是如何区分的 算法收敛的速度 SVM定义了函数距离\n$\\rho_f(x, y) = y \\times f(x)$\n$min\\rho_f = \\rho_{min} = min(\\rho_f(x_i, y_i))$\n目标就是找到一个模型满足:\n$f* := argmax_f\\rho_{min} = argmax_f min\\rho_f(x_i,y_i)$\n实际上也就是 $y = w^Tx + b$, 寻找一组参数$w^* = (w,b)$, 使得$\\rho = min(y_i (\\frac{w\\cdot x}{|w|} + \\frac{b}{w}))$ 最大(间隔最大)\n当我们的$\\rho \u0026gt; 0$时, SVM是硬间隔支持向量机。\n以上是基本的SVM模型, 而如何更好地求解SVM引入了对偶形式地SVM\n原问题\n$argmax_{w,b}\\ \\rho,\\ s.t.\\ \\frac{y_i(w^Tx+b)}{|w|} \\geq \\rho$\n等价于\n标准形式下(最近距离$\\rho = 1$)的目标函数 $min_w{ \\frac{1}{2} |w|^2},\\ s.t.\\ \\forall_{i = 1}^n y_i(w^Tx_i + b) \\geq 1$ 引入Lagrange乘数项\n转为求解 $L(w,b,\\alpha) = \\frac{1}{2}|w|^2 - \\sum_{i = 1}^n\\alpha_i (y_i(w^Tx_i + b) - 1))$ 求导数为0得到中间结果\n$w = \\sum_{i = 1}^n \\alpha_i y_i x_i$\n$\\sum_{i =1}^n\\alpha_i y_i = 0$\n代入原式 $L(w,b,\\alpha) = \\frac{1}{2}|w|^2 - \\sum_{i = 1}^n\\alpha_i (y_i(w^Tx_i + b) - 1))$\n得到化简结果(w化简自然引入内积, 这也是后面非线性/核方法的重要基础)\n$L(\\alpha) = \\sum_{i = 1}^n\\alpha_ - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n y_iy_j \\alpha_i \\alpha_j (x_i \\cdot x_j)$\n使得$\\alpha_i \\geq 0,\\ 1\\leq i \\leq n$成立;\n以及 $\\sum_{i = 1}^n \\alpha_i y_i = 0$\n求出$\\alpha$ 得到了 $w^*$\n观察$f(x) = w^Tx + b = \\sum_{i = 1}^n \\alpha_i y_i x_i^T x + b$\n对于所有支持向量$x_j$满足$y_jf(x_j) = 1$\n也就是说$f(x_j) = \\sum\\alpha_i y_i (x_i \\cdot x_j) + b$\n$b^* = \\frac{1}{N_s}\\sum_{j\\in S}(y_j - \\sum_{i\\in S} \\alpha_i y_i x_i \\cdot x_j)$ (其实就是所有支持向量的均值)\n于是我们的判别函数 $f(x) = sgn(\\sum_{i = 1}^n\\alpha_i^* y_i x_i^T x + b^* )$就求出来了;\nNon-Linear SVM 非线性支持向量机 $L(\\alpha) = \\sum_{i = 1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n y_iy_j \\alpha_i \\alpha_j (x_i \\cdot x_j)$\n这里将原先的线性SVM内积换成核函数$K(x_i, x_j)$\n得到代入核函数的内积\n$L(\\alpha) = \\sum_{i = 1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n y_iy_j \\alpha_i \\alpha_j (K(x_i, x_j))$ 使得$\\alpha_i \\geq 0,\\ 1\\leq i \\leq n$成立;\n以及 $\\sum_{i = 1}^n \\alpha_i y_i = 0$\n最终我们的分类函数可以表示为:\n$f(x) = \\sum_{i = 1}^n\\alpha_i^* y_i K(x_i, x) + b^*$\n为了能够更快求解Kernel-product, 我们对于给定的函数$k: X^2 \\to K$, 以及原空间的向量模式$(x_1,\u0026hellip;,x_n)\\in X$\nGram矩阵$K_{ij} = k(x_i,x_j)$(内积矩阵)\n于是就可以更快求解上面的目标式。\n常用的核函数\n高斯核函数 $k(x,x\u0026rsquo;) = e^{-\\frac{|x-x\u0026rsquo;|^2}{2\\sigma^2}}$\n多项式核函数 $k(x,x\u0026rsquo;) = (x^T x + c)^d$\n4.实验结果分析 # 4.1评价指标 # MNIST的数据集 评价指标自然是分类的准确率 $p$ (分类正确个数/总测试样本数)\nBoston房价数据集 由于是一个回归问题, 合理的评价标准是偏离度/SSE\n这里采用 MSE(Mean Squared-Error) 以及确信度(也叫R^2系数) 4.2定量评价结果 # 1. MNIST分类结果 # 使用SVM/非线性SVM对于手写数字进行分类。\n辅助函数\nshow_pics 用于展示几张经过模型学习过的图片样式 和 实际的结果 submit_result 用于提交我们的SVM模型预测结果到kaggle-submission 基本的线性SVM # 经过基本的SVM模型学习后，我们来预测几张图片观察以下模型的效果\n可以看到正确率已经有了80%以上，但是还是有很明显的错误识别(比如右上角的'3\u0026rsquo;)被识别成了'2';\n非线性的SVM # 2. Boston预测准确率 # 线性SVM的实验结果 # 具体的分类器代码实现 最终的预测结果与实际数据之间的MSE大小为$25.38$ 确信度(R-squared) 系数为 $0.659$ 多项式核函数SVM # 具体的分类器代码实现\n经过试验后，挑选的多项式核函数度数为$3$, 正则项系数$C=10$ 最终的预测结果与实际数据之间的MSE大小为$16.47$ 确信度(R-squared) 系数为 $0.78$ 径向基函数SVM # 具体的分类器代码实现如下所示\nSVR的默认实现就是 rbf RBF（Radial Basis Function，径向基函数）是一个函数空间中的基函数，而这些基函数都是径向函数。\n径向函数: 满足对于围绕着某固定点 $c$ 的等距的 $x$，​函数值相同 $$ \\phi(x) = \\phi(||x-C||) $$\n使用rbf回归的svr代码：\n最终的预测结果与实际数据之间的MSE大小为$13.82$ 可以看到较线性的SVR提升很大 确信度(R-squared) 系数为 $0.82$ 4.3可视化结果 # MNIST分类结果 使用最简单的线性SVM提交测试结果 即使是最简单的SVM，也已经可以达到86%左右的正确率了\n使用多项式核函数的SVM 代码实现:\nkernel挑选为多项式核 测试的多项式次数为3 正则项系数$C$的大小为10 后来经过了一系列的挑选和尝试，最终设置了SVC的多项式核函数度数为2\n度数的选择 发现选择度数 = 2好像更好;\n代码: 结果: 最终选择多项式核函数的SVM\n预测结果:\n预测的准确率达到了$0.95$附近的水平，已经很高了； 这也说明原先的数字分类问题不是一个线性可分的分类问题。 Boston预测准确率 最终的实验结果和分布曲线拟合情况如下所示：\n线性SVR的实验结果 使用rbf核函数的SVR 多项式核函数的SVR\n5.总结 # 比较不同方法的优缺点等。 SVM总结:\nSVM算法是用于分类问题非常实用的一种算法；\nSVM的优势在于解决小样本、非线性和高维的回归和二分类问题； 由于支持向量的数量其实占整个数据集合的比例通常不会很大，因此SVM可以用于选取数据集中的代表元，还可以用于缩小问题规模； 核函数方法扩展的SVM还可以解决非线性的分布下的分类问题，使得SVM更为实用。 使用核函数的SVM(SVR)\n常用的核函数有多项式核函数(poly-kernel), rbf 径向基函数, Gaussian核函数等等； 核函数的技巧使得原先只能处理线性问题的SVM可以在更高的维度中处理原输入空间的非线性问题。 ","date":"2022 年 11 月 6 日","externalUrl":null,"permalink":"/posts/2022/svm-svr/","section":"Posts","summary":"","title":"支持向量机/向量机回归总结(SVM-SVR)","type":"posts"},{"content":"placeholder replace me!\n机器学习三要素\n映射函数 目标函数 设计算法 按照基本的分类\n分类算法 回归算法 重点\n算法的设计, 物理含义, 适用场景; 映射函数/损失函数的设计, 公式含义; 期中复习 # 分类算法 # 线性模型分类器\n分类 与 回归 的区别 回归: 对于每一个输入$x$ 输出一个预测$y$为实数值;\n分类: $X-\u0026gt;Y = C_k$ 也就是映射出的值是一个标签(K = 2, K \u0026gt; 2)的情况\n1-of-K 编码 (1, 0, 0, 0, 0) - (0, 0, 0, 0, 1) 输入空间被划分成 决策区域，边界就是决策平面/决策边界\n判别函数 $F(X) = C_K$ discriminant function\n或者 概率函数$P(C_K|X)$ 指的是一个输入属于某一类别的概率大小\n基于概率的决策分类模型: 对于$P(C_K | X)$进行概率预测, 建模\n或者 生成模型\n$P(X|C_K)$ 我们的预测值,给定标签$C_K$ 问出现输入为$X$的概率是多少?\n$P(C_K) $ 先验已知\n利用贝叶斯 推算后验概率 $P(C_K | X) = \\frac{P(X|C_K)P(C_K)}{P(X)}$\n机器学习的两类模型: 判别模型和生成模型\n基础线性模型 $y(x) = w^Tx+b$ $y(x) = f(w^Tx + b)$\n泛化的线性模型- 分布在$(0, 1)$之间\n$f(.)$是激活函数\n决策平面仍然是x的线性函数,即使激活函数非线性 分类算法整理(Classification) # 输入变量$x$, 模型$y = f(w,x)$, 输出$y_k \\in {C_1, \u0026hellip;, C_k}$ 离散值\n1. 感知机 Perceptrons # 模型\n单层神经元模型\n模型可以表示为一个广义上的线性模型(关于基函数basis-function)\n$x$为我们的输入变量, $\\phi(x)$为基函数, $w$为待学习的参数 $$ y(x) = f(w^T\\phi(x)) $$\n这里的非线性激活函数 $f(*) = sgn(x)$ 也就是符号阶跃函数 即 $f(a) = 1, a \\geq 0;\\ f(a) = -1, a \\lt 0$\n评价标准\n(判别模型)错分的实例数量 (生成模型)概率分布 例如输出变量$t \\in {-1, +1}$ 表示二分类的$C_1, C_2$\n那么有$1 = t = w^T\\phi(x), t\\in C_1, t \u0026gt; 0$\n否则$-1 = t \u0026lt; 0$\n误差函数\n定义$\\phi_i = \\phi(x_i)$ 即对应第$i$个实例向量 $E(w) = -\\sum_{\\phi_i \\in M} w^T \\phi_i t_i$ ，实际上就是错分类的数量 对这个函数进行梯度下降可以求解最优解；\n学习过程：SGD\n$w\u0026rsquo; = w - \\eta \\frac{\\partial E}{\\partial w} = w + \\eta \\phi_i t_i$\n如果存在错误实例/$E(w)$大于0，就不断更新$w$\n收敛性证明: 数据实例线性可分情况下始终会收敛\n简单的证明:\n当前的损失 $$E(w) = -w\\phi(x_i)y_i$$\n更新后 $$ E(w\u0026rsquo;) = -w\u0026rsquo;\\phi(x_i)y_i = -(w+\\eta \\phi_i t_i)\\phi(x_i)y_i = E(w) - \\eta (\\phi_i y_i)^T \\phi_i t_i $$\n而后面这一项由定义可知总是非负数;因此如果存在最优解总是会在有限时间内收敛.\n总结 # 优点: 简单，对线性可分问题总是可求解，可扩展； 缺点：受数据影响大，无法支持非线性可分问题，学习率选取？ 2. KNN # 模型 # 输入: 训练数据集 $$ T = {(x_1,y_1), \u0026hellip;, (x_n, y_n)} $$\n输出: 实例 $x$ 所属的类$y$\n根据给定的距离计算方式，在T中寻找$x$最近的k个点，这个区域记作$N_k(x)$ 在区域$N_k(x)$内进行选择，取最多的出现标签 所以得出最终的推测结果$y$\n$$ y = argmax_{c_j} \\sum _{x_i \\in N_k(x)} I(y_i = c_j) $$\n误差函数 # 距离度量的选择 L1距离 $L_1(x_i,x_j)$\nL2距离 $L_2(x_i,x_j)$\nLp距离 == Minkowski距离 $L_p(x_i,x_j) = (\\sum_{l=1}^n |x_i^l - x_j^l|^p)^{\\frac{1}{p}}$\nk值大小的选择 较小的$k$: 敏感 容易被噪声点影响, 学习的近似误差会减小；但是估计误差会增大\n较大的$k$: 有效减小学习的估计误差，但是近似误差会变大\n多数表决原则 错误分类的概率 $P(Y \\neq F(x)) = 1 - P(Y = F(x))$\n给定领域$N_k(x)$ 那么错误分类的概率等于 $1 - \\frac{1}{k}\\sum_{x_i \\in N_k(x)} I(y_i = c_j)$\n为了尽可能缩小错分概率 == 尽可能增大分类正确的概率\nKNN的实现-kd tree # 给定一个目标点，首先搜索最近邻，找到包含目标点的kd-node，然后依次回退到它的parent节点，不断寻找与目标点距离最近的节点直到找不到更近的节点。\nkd-node 每一个节点对应了k-dimension种某一个维度的二分区域结果，叶子节点中只包含一个实例点\n在kd树上搜索(K)最近邻可以高效利用节点的性质完成\n3. 朴素贝叶斯 # 模型在给出已知的样本训练集分布$p(x)$以及此时的先验输出概率$p(y)$, 试图学习条件概率$p(y|x)$的分布模型, 使得其表现的最好, 这就是贝叶斯学习的基本想法.\n4. Logistic回归(最大熵模型) # 逻辑斯蒂回归-分类常用的算法\n引入：bayes公式=\u0026gt; logistc函数 $$ P(C_1 | x) = \\frac{p(x|C_1) P(C_1)}{p(x|C_1)P(C_1) + p(x|C_2)P(C_2)} = \\frac{1}{1 + e^-a} = \\sigma(a) $$\n那么求解$a = ln \\frac{p(x|C_1)P(C_1)}{p(x|C_2)P(C_2)}$ (后面可以看到实际上就是几率)\n模型 # logistic函数 $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n对于二分类问题 定义几率 $$ logit = \\frac{p(c_1)}{p(c_2)} = \\frac{p(c_1)}{1 - p(c_1)} $$\nlogistic函数的反函数就是 ln几率\n$x = ln(\\frac{\\sigma(x)}{1 - \\sigma(x)})$\nlogistic函数作为激活函数: 使得我们的判别结果可以理解为后验概率 多分类问题下的logistic函数\n标准化 exp/softmax函数 $$ P(C_k|x) = \\frac{p(x|C_k) P(C_k)}{\\sum_j p(x|C_j)P(C_j)} = \\frac{exp(a_k)}{\\sum_j exp(a_j)}, \\ where\\ a_k = ln\\ p(x|C_k)P(C_k) $$\n以上讨论的都是离散取值的情况，对于连续的情况，我们假设样本关于标签取值是正态分布\n评价标准 # 定义误差函数(交叉熵)\n对于一个数据集${x_n, t_n}$, $t_n \\in {0,1}$\n输出为标签$t$的概率似然函数可以写作是: $P(t|w) = \\Pi_{n=1}^N y_n^{t_n} (1 - y_n)^{1 - t_n}$ 也就是每一个分量取到正确值的概率之积\n损失函数就是它的负对数:\n$E(w) = -ln P(t|w) = -\\sum_{n=1}^N(t_n ln y_n + (1 - t_n) ln (1 - y_n))$ 也被称为交叉熵误差，其中$y_n = \\sigma(a_n) = \\sigma(w^Tx + b) = \\sigma(w^T\\phi)$\n总结 # logistic函数与对数几率;\n误差函数定义为交叉熵损失($-ln P(t|w)$), 对后验概率的对数取负数;\n多分类情况 =\u0026gt; softmax函数\n$P(t|w) = \\sigma(a) = \\sigma(w^Tx+b)$ 可以看出对数几率 关于输入$x$是一个线性函数，这也就是模型的本质\n5. 决策树* # 6. SVM(线性, 核方法) # SVM-支持向量机\n输入: 样本集合 $T = {(x_1, y_1), \u0026hellip;, (x_n, y_n)}$, $x$为输入变量, $y$为输出值(eg. 属于的类别)\n输出: 我们的SVM模型 $y = w^Tx + b$\n$w^T = w^{\\star}$, $b = b^{\\star}$\n满足条件 $maxmin(f(x_i)) = max(min|{w^{*}}^{T}x + b|)$\n其中 我们记录$\\rho = |w^{T}x + b|$ 为样本点的绝对距离, 由于此时我们的系数$w^$的模大小是1, 因此实际的距离$y_if(w^Tx+b)/|w|$也就是上式的值(由于我们是一个二分类问题, $y_i$取值为${1, -1}$)\n映射函数: $y = f(x_i) = w^Tx + b$\n目标: 最大化 样本点到 决策平面的最小距离, 此时落在这个最小距离上的样本被称为支持向量\n目标函数 $min_w{ \\frac{1}{2} |w|^2},\\ s.t.\\ \\forall_{i = 1}^n y_i(w^Tx_i + b) \\geq 1$\n要证明我们的分类器SVM能够收敛, 我们这里沿用感知机模型的结论(感知机不要求这个函数距离的最小, 只要求分类正确因此可以存在很多个)\n收敛性, 最多经过$(b^2 + 1)(R^2 + 1) / \\rho^2$ 次更新,就能找到答案($R = max_i ||x_i||$)\nSVM的重要参数: $\\rho$ 几何距离大小(当$|w| = 1$时也就是函数距离)\n$\\rho$(margin) 决定了:\n两个类是如何区分的 算法收敛的速度 SVM定义了函数距离\n$\\rho_f(x, y) = y \\times f(x)$\n$min\\rho_f = \\rho_{min} = min(\\rho_f(x_i, y_i))$\n目标就是找到一个模型满足:\n$f* := argmax_f\\rho_{min} = argmax_f min\\rho_f(x_i,y_i)$\n实际上也就是 $y = w^Tx + b$, 寻找一组参数$w^* = (w,b)$, 使得$\\rho = min(y_i (\\frac{w\\cdot x}{|w|} + \\frac{b}{w}))$ 最大(间隔最大)\n当我们的$\\rho \u0026gt; 0$时, SVM是硬间隔支持向量机。\n以上是基本的SVM模型, 而如何更好地求解SVM引入了对偶形式地SVM\n原问题\n$argmax_{w,b}\\ \\rho,\\ s.t.\\ \\frac{y_i(w^Tx+b)}{|w|} \\geq \\rho$\n等价于\n标准形式下(最近距离$\\rho = 1$)的目标函数 $min_w{ \\frac{1}{2} |w|^2},\\ s.t.\\ \\forall_{i = 1}^n y_i(w^Tx_i + b) \\geq 1$ 引入Lagrange乘数项\n转为求解 $L(w,b,\\alpha) = \\frac{1}{2}|w|^2 - \\sum_{i = 1}^n\\alpha_i (y_i(w^Tx_i + b) - 1))$ 求导数为0得到中间结果\n$w = \\sum_{i = 1}^n \\alpha_i y_i x_i$\n$\\sum_{i =1}^n\\alpha_i y_i = 0$\n代入原式 $L(w,b,\\alpha) = \\frac{1}{2}|w|^2 - \\sum_{i = 1}^n\\alpha_i (y_i(w^Tx_i + b) - 1))$\n得到化简结果(w化简自然引入内积, 这也是后面非线性/核方法的重要基础)\n$L(\\alpha) = \\sum_{i = 1}^n\\alpha_ - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n y_iy_j \\alpha_i \\alpha_j (x_i \\cdot x_j)$\n使得$\\alpha_i \\geq 0,\\ 1\\leq i \\leq n$成立;\n以及 $\\sum_{i = 1}^n \\alpha_i y_i = 0$\n求出$\\alpha$ 得到了 $w^*$\n观察$f(x) = w^Tx + b = \\sum_{i = 1}^n \\alpha_i y_i x_i^T x + b$\n对于所有支持向量$x_j$满足$y_jf(x_j) = 1$\n也就是说$f(x_j) = \\sum\\alpha_i y_i (x_i \\cdot x_j) + b$\n$b^* = \\frac{1}{N_s}\\sum_{j\\in S}(y_j - \\sum_{i\\in S} \\alpha_i y_i x_i \\cdot x_j)$ (其实就是所有支持向量的均值)\n于是我们的判别函数 $f(x) = sgn(\\sum_{i = 1}^n\\alpha_i^* y_i x_i^T x + b^* )$就求出来了;\nNon-Linear SVM 非线性支持向量机 $L(\\alpha) = \\sum_{i = 1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n y_iy_j \\alpha_i \\alpha_j (x_i \\cdot x_j)$\n这里将原先的线性SVM内积换成核函数$K(x_i, x_j)$\n得到代入核函数的内积\n$L(\\alpha) = \\sum_{i = 1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n y_iy_j \\alpha_i \\alpha_j (K(x_i, x_j))$ 使得$\\alpha_i \\geq 0,\\ 1\\leq i \\leq n$成立;\n以及 $\\sum_{i = 1}^n \\alpha_i y_i = 0$\n最终我们的分类函数可以表示为:\n$f(x) = \\sum_{i = 1}^n\\alpha_i^* y_i K(x_i, x) + b^*$\n为了能够更快求解Kernel-product, 我们对于给定的函数$k: X^2 \\to K$, 以及原空间的向量模式$(x_1,\u0026hellip;,x_n)\\in X$\nGram矩阵$K_{ij} = k(x_i,x_j)$(内积矩阵)\n于是就可以更快求解上面的目标式。\n常用的核函数\n高斯核函数 $k(x,x\u0026rsquo;) = e^{-\\frac{|x-x\u0026rsquo;|^2}{2\\sigma^2}}$\n多项式核函数 $k(x,x\u0026rsquo;) = (x \\cdot x\u0026rsquo; + c)^d$\nMLP 多层感知机/Feed Forward networks MLP的结构:\n输入层 隐层 输出层 特点:\n输入层大小为输入向量的维数\n隐层之间的神经元是全连接的\n隐层神经元的激活函数是非线性的\n对于网络中的第j个神经元而言, 它所收到的响应为 $net_j = \\sum_{i = 1}^d w_{ji} x_i + w_{j0} = \\sum_{i = 0}^d w_{ji} x_i = w_j^T \\cdot x$\n因此, 第j个神经元的输出就是$y_j = g(net_j)$\n对于输出层的第$k$个神经元, 它的信号相应就是$net_k = \\sum_{j = 1}^{n_H} w_{kj} y_j + w_{k0} = \\sum_{j=0}^{n_H}w_{kj} y_j = w^T_k \\cdot y$\n最终对于标签分类为$k$的输出就是$y_k = f(net_k) = f(w^T_k \\cdot y) = f(\\sum_{j = 0}^{n_H} w_{kj} f(net_j)) = f(\\sum_{j = 0}^{nH} w_{kj} f(\\sum_{i = 0}^d w_{ji} x_i))$\n对于二分类问题而言, $y_k \\in {1, -1}$\n对于多分类问题, $y_k = f(net_k) = g_k(x)$\nMLP采用的不同Loss function\nSSE Sum of Squared Loss 物理意义上, 就是实际输出与我们预测的输出之间的方差\n$$ E(w) = \\frac{1}{2} \\sum_{n = 1}^N (y(w, x_n) - t_n)^2 $$\n概率上的解释: 网络输出给出了一种概率上的分布, 使用概率的好处 1. 可以引入非线性 2. 更多的loss选择 从概率论的角度分析\nNN学习到的是一个关于输出变量$t$的概率分布\n$P(t|w, x) = N(t| y(w,x), \\beta^{-1})$\n假设初始的输入数据$x$关于输出标签$t$的取值是一个高斯分布, 期望$\\mu = y(w,x)$ 方差为$\\beta^{-1}$ 假设我们的输入是独立同分布的, 那么对于所有的标签$t=(t_n)_{n=1}^N$ 我们列出似然函数\n$P(t|w,x,\\beta) = \\Pi_{n=1}^n P(t_n|w,x_n, \\beta)$\n$P(t_n | w, x_n, \\beta) = \\frac{\\beta}{\\sqrt 2\\pi} exp(\\frac{-\\beta}{2}(t_n - y(w,x_n)^2))$\n对数似然函数就是 $L = \\sum_{n=1}^N ln(P(t_n | w,x_n,\\beta)) =\\frac{N}{2}(ln\\beta - ln 2\\pi) - \\frac{\\beta}{2}(y(w,x_n) - t_n)^2 $\n让对数似然函数最大 等价于 让 $\\frac{\\beta}{2}(y(w,x_n) - t_n)^2$ 最小化,正好就是平方误差SSE最小化!\n因此从概率角度我们发现概率最大 == 误差最小, 可以转变为一个学习概率分布的问题\n使用的误差函数有:\nCross-Entrophy Loss 交叉熵\n对于一个二分类问题而言\n$t$是输出变量;\n$t = 1$对于$C1$; $t = 0$对于$C2$; 网络有单个输出, 输出激活函数是\n$$ y = \\sigma(a) = \\frac{1}{1 + e^{-a}} $$\n那么输出为$t$的概率是: $$ P(t|X, w) = y(x, W)^t (1 - y(x, W))^{1 - t} $$\n相应的误差函数\n$$ E(w) = -\\sum_{n = 1}^N{t_n ln y_n + (1 - t_n) ln(1 - y_n) } $$\nK分类 输出激活函数为 logistic函数\n输出标签值为$t$的概率就是 $$ P(t|w,x) = \\Pi_{k=1}^K y_k(w,x)^{t_k} (1 - y_k(w, x))^{1 - t_k} $$ 对应的损失函数 $$ E(w) = -\\sum_{n=1}^N\\sum_{k=1}^K{t_{nk} ln y_{nk} + (1 - t_{nk}) ln (1 - y_{nk})} $$\n1-of-K coding\n不断地用二分类的方式区分标签值为k的概率与非k的概率 输出$y_k$的概率公式: $y_k(w,x) = \\frac{exp(a_k(w,x))}{\\sum_j exp(a_j(w,x))}$ 多层感知机的BP算法\nError BackPropagation\n从隐藏层到输出层的学习\n由于输出层 第$k$个单元的响应为\n$net_k = \\sum_{j=0}^{n_H} w_{kj} y_j$\n所以对于给定的$E(w)$, 关于隐层到输出层系数的梯度\n$$ \\frac{\\partial E}{\\partial w_{kj}} = \\frac{\\partial E}{\\partial net_k} \\frac{\\partial net_k}{\\partial w_{kj}} = -\\delta_{k} \\frac{\\partial net_k}{\\partial w_{kj}} $$\n$\\delta_k$是第$k$个单元的sensitivity(敏感度) 假设激活函数f是可微分的\n$$ \\delta_k = \\frac{\\partial E}{\\partial net_k} = \\frac{\\partial E}{\\partial y_k} \\frac{\\partial y_k}{\\partial net_k} =(t_k - y_k) f\u0026rsquo;(net_k) $$\n(因为这里的误差函数就是SSE)\n又因为 $net_k = w_{kj}^T \\cdot y_j$ 所以$\\frac{\\partial net_k}{\\partial w_{kj}} = y_j$\n最终得到隐层-输出层的梯度学习规则\n$$ \\Delta_{kj} = \\eta \\delta_k y_j = \\eta (t_k - y_k) f\u0026rsquo;(net_k) y_j $$\n其中$\\eta$是我们定义的学习率\n从输入层到隐藏层的学习 第$j$个神经元的响应$net_j = \\sum_{i=0}^d w_{ji}x_i$\n而根据我们的结构, 可以列出以下的梯度关系 $$ \\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial net_j} \\frac{\\partial net_j}{\\partial w_{ji}} $$\n由于 $\\frac{\\partial net_j}{\\partial w_{ji}} = x_i$ 以及 $\\frac{\\partial y_j}{\\partial net_j} = f\u0026rsquo;(net_j)$\n还有误差函数关于输出y的导数 $$ \\frac{\\partial E}{\\partial y_j} = \\frac{\\partial}{\\partial y_j}[\\frac{1}{2}\\sum_{k=1}^c (t_k - y_k)^2] = -\\sum_{k=1}^c(t_k-y_k) \\frac{\\partial y_k}{y_j} = -\\sum_{k=1}^c(t_k-y_k) \\frac{\\partial y_k}{net_k} \\frac{\\partial net_k}{y_j} = -\\sum_{k=1}^c(t_k-y_k) f\u0026rsquo;(net_k) w_{kj} $$\n类似的我们定义一个隐层单元的敏感度 $\\delta_j = f\u0026rsquo;(net_j) \\sum_{k=1}^c w_{kj} \\delta_k$\n那么对于输入层-隐层的学习规则我们得到\n$$ \\Delta w_{ji} = \\eta \\delta_j x_i = \\eta f\u0026rsquo;(net_j) [\\sum_{k=1}^c w_{kj}\\delta_k] x_i $$\nMLP在实际应用中需要考虑的问题\n激活函数的选择: 非线性/线性, 单调性, 连续性\n隐层的神经元数量 $n_H \\to n/10$\n初始网络的权重\n学习率大小$\\eta = 0.1$\n权重的衰减\n隐层的数量\n目标函数的选择\n回归算法整理 # 1. Linear Regression # 输入: 训练数据集$T = {(x_1,y_1), \u0026hellip;, (x_n, y_n)}$\n输出: regression-function 回归函数 $y = f(w,x) = w^Tx + b$\n目标: 最小化误差函数 例如MSE, $min_w E(w) = \\frac{1}{n} |y(w,x) - y|^2$\n找到的最优参数 $w^* = argmin_{w}\\sum_{i=1}^n(y(w,x) - y)^2$\n寻找最优参数的方法:\nSGD 随机梯度下降;\n解析法: 最小二乘解$(X^TX)^{-1}X^Ty$\n2. Lasso, Ridge # Ridge\n目标: 最小化误差函数 $L = \\sum_{i=1}^n (y_i - w^Tx_i)^2 + \\lambda ||w||^2$\n$w^* = argmin_w (\\sum_{i=1}^n (y_i - w^Tx_i)^2 + \\lambda ||w||^2)$ 解析算出的最优解$\\boldsymbol{w} = (X^TX + \\lambda I)^{-1} X^T y$ 或者用SGD\nLasso\n目标: 最小化误差函数 $L = \\sum_{i=1}^n (y_i - w^Tx_i)^2 + \\lambda ||w||_1$\n最优参数 $w^* = argmin_w (\\sum_{i=1}^n (y_i - w^Tx_i)^2 + \\lambda ||w||_1)$ 注意次数我们不能通过求导数的方式得到最优解 需要用 坐标下降的方法\n随机初始化系数 $\\boldsymbol{w}$ 遍历各个维度的$w_i$, 固定其余的$w_j, (i \\neq j)$, 将$w_i$视为变量求出最优解 迭代上一个步骤直到各个维度的参数都不变化(或者达到最大迭代次数) 3. SVR # SVR是SVM的一个重要应用.\nSVR所寻找的最优hyper-plane目标是 \u0026ldquo;使得所有的样本点距离超平面的偏差最小\u0026rdquo; {% asset_img svm.png \u0026ldquo;SVM/SVR图示\u0026rdquo; %}\nSVM/SVR图示 线性SVR # 输入: $T = {(x_1,y_1), \u0026hellip;, (x_n,y_n) }$\n输出: 回归预测结果$\\hat y = (w^Tx + b)$\n模型: 线性回归函数$y(w,b) = w^Tx + b$\n目标函数:\n这里是两边采用了不同的松弛程度($\\xi, \\xi^{\\star}$)\n$$ min_{w,b,\\xi} \\frac{1}{2} ||w||^2 + C \\sum_i (\\xi_i+ \\xi_i^*) \\ s.t.\\ y_i - (w^Tx_i + b) \\leq \\epsilon + \\xi_i \\ (w^Tx_i + b) - y_i \\leq \\epsilon + \\xi_i^{\\star} \\xi_i, \\xi_i^{\\star} \\geq 0,\\ i = 1,2,\u0026hellip;,n $$\n损失函数: 0-1 Loss =\u0026gt; 代理损失函数 surrogate loss\nhinge_loss: $l(z) = max(0, 1 - z)$ 指数损失 对数损失 以线性SVR, 两边的误差间距不一样的情况为例, 求解其对偶问题\n因为我们引入了4个约束条件, 所以引入4个乘子$\\mu_i, \\mu_i^{\\star}, \\alpha_i, \\alpha_i^{\\star}$\n$$ L(w,b,\\alpha,\\alpha^{\\star},\\xi,\\xi^{\\star},\\mu,\\mu^{\\star}) = \\frac{1}{2} |w|^2 + C\\sum_{i=1}^m(\\xi_i + \\xi_i^*) - \\sum_{i=1}^m \\mu_i \\xi_i - \\sum_{i=1}^m \\mu_i^{\\star} \\xi_i^{\\star} + \\sum_{i=1}^m(\\alpha_i ((w^Tx_i + b) - y_i - \\epsilon - \\xi_i)) + \\sum_{i=1}^m \\alpha_i^{\\star} ((w^Tx_i + b) - y_i - \\epsilon - \\xi_i^{\\star}) $$\n再去求解拉格朗日乘数函数的最小值: $L$是凸优化问题, 求解偏导数等于0\n得到 $$ w = 1 \\ 0 = \\sum_{i=1}^m(\\alpha_i^{\\star} - \\alpha_i) \\ C = \\alpha_i + \\mu_i \\ C = \\alpha_i^{\\star} + \\mu_i^{\\star} \\ $$\n拉格朗日对偶性 # $$ min f(x) \\ s.t.\\ c_i(x) \\leq 0\\ i = 1,2,\u0026hellip;,k (k个约束条件) \\ h_j(x) = 0,\\ j = 1,2,\u0026hellip;,l (l个约束条件) $$\n引入广义的的拉格朗日函数(对于约束分别引入乘数项)\n$L(x,\\alpha, \\beta) = f(x) + \\sum_{i=1}^k \\alpha_i c_i(x) + \\sum_{j=1}^l \\beta_j h_j(x) $\n考虑关于$x$的函数: $\\Theta_p(x) = max_{\\alpha,\\beta,\\alpha_i \\geq 0} L(x, \\alpha, \\beta)$\n那么如果$x$可以满足约束, 那么$\\Theta_p(x) = f(x)$\n否则必然存在某个约束条件不满足, 取相应的系数$\\alpha_i$或者$\\beta_j$使其取到$\\inf$, 那么$\\Theta_p(x) = +\\inf$\n所以我们考虑关于$\\Theta_P(x)$的极小化问题 $$ min_x \\Theta_P(x) = min_x max_{\\alpha, \\beta, \\alpha_i \\geq 0} L(x, \\alpha, \\beta) $$\n它与原始问题$min_{x\\in R} f(x)$是等价的, 所以我们把原始问题的最优化问题表示为了拉格朗日函数的极小-极大问题;\n$p^{\\star} = min_x \\Theta_P(x)$ 也就是原问题的值.\n对偶问题: $\\Theta_D(\\alpha, \\beta) = min_x L(x,\\alpha, \\beta)$\n极大化对偶问题等价于 $max_{\\alpha,\\beta,\\alpha_i \\geq 0} min_x L(x,\\alpha, \\beta)$ 就是拉格朗日函数的 极大-极小问题;\n对偶问题的值 $d^{\\star} = max_{\\alpha,\\beta,\\alpha_i \\geq 0} \\Theta_D(\\alpha, \\beta)$\n考虑原始问题与对偶问题的关系, 可以发现$d^{\\star} \\leq p^{\\star}$\n在满足上述所有的条件的解,成立 $d^{\\star} = p^{\\star}$\n于是就完成了原问题(min) 到 拉格朗日问题(min-max) 再到 拉格朗日对偶问题(max-min)的转化.\n补充:\nKKT条件\n$\\frac{\\partial L}{\\partial x} = 0$ $\\alpha_i^{\\star} c_i(x^{\\star}) = 0$ $c_i(x^{\\star}) \\leq 0$ $\\alpha_i^{\\star} \\geq 0$ $h_j(x^{\\star}) = 0$ 4. LDA() # 问题：需要进行输入变量的投影(eg 从高维度输入转为一个低维度，必然会丢失一些特征)\n例如以下的线性映射:\n$x\\in R^d \\to y \\in R,\\ y = w^Tx$\n选择一个最好的权重$w$使得投影后的结果能最大限度区分不同类别。 考虑一个二分类的问题\n$N_1, C_1$, $N_2, C_2$为两类的点数和类别的输出；\n那么在原始空间的平均值向量\n$$ m_k = \\frac{1}{N_k}\\sum_{i\\in C_k} x_i $$\n在投影空间的平均值向量 $$ \\mu_k = \\frac{1}{N_k}\\sum_{i\\in C_k}w^T x_i $$\n那么为了区分类别，我们需要可以区分投影后类别的均值，于是我们希望最大化均值之间的绝对差值\n$$\\mu_2 - \\mu_1 = w^T(m_2 - m_1)$$\n又有问题: $w$的选取随机性很大\n解决: 单位长度 $||w|| = 1$ 通过lagrange乘数法进行约束条件下的最大化求解$w$, 于是我们可以求出$w$ Fisher Linear Discriminant\n希望在两类的均值差值大的情况下, 让一个类内部的分布更可能集中(方差小)\n类间均值差距 $\\mu_2 - \\mu_1$\n类内方差 $\\sum_{i\\in C_1}(w^Tx_i - \\mu_1)(w^Tx_i - \\mu_1)^T + \\sum_{i\\in C_2}(w^Tx_i - \\mu_2)(w^Tx_i - \\mu_2)^T = \\sigma_1 + \\sigma_2$\nFisher Linear Discriminant目标函数 $J(w) = \\frac{(\\mu_2 - \\mu_1)^2}{\\sigma_1 + \\sigma_2}$\n$$ J(w) = \\frac{w^TS_Bw}{w^TS_ww} $$ 其中$S_B$为类间散度矩阵, $S_w$为类内散度矩阵\n$S_B = (m_2 - m_1)(m_2-m_1)^T$, $S_w=\\sum_k\\sum_{i\\in C_k} (x_i - m_k)(x_i - m_k)^T$\n5. Polynomial regression # Radix-Based Regression 基向量(分量) 其他:\nLeast-Square Error的问题\n样本分布问题=\u0026gt;扰动很大\nLeast squares corresponds to maximum likelihood under the assumption of a Gaussian conditional distribution.\n","date":"2022 年 11 月 5 日","externalUrl":null,"permalink":"/posts/2022/ml-notes/","section":"Posts","summary":"\u003cp\u003eplaceholder replace me!\u003c/p\u003e","title":"ml_notes","type":"posts"},{"content":" 下午茶合集 # 10.17 767C # 树上, 一次DFS求出子树的和/子树的点权和, 必须掌握的套路\n无关: 可以在线跑任务的平台:\ncolab google 需要科学 需要外币 autodl 国内的 不知道收费怎么样 10.18 1420D # https://codeforces.com/problemset/problem/1420/D\n输入 $n, k(1\\lt k \\lt n \\lt 3e5)$ 和 $n$ 个闭区间，区间的范围在 $[1,1e9]$。 你需要从 $n$ 个区间中选择 $k$ 个区间，且这 $k$ 个区间的交集不为空。 输出方案数模 $998244353$ 的结果。\n对这些区间排序，比如按照左端点排序\n选定其中的第$i$个区间，那么在它之后的，能够交集不为空的区间的左端点 小/等于 区间$i$的右端点， 在它之前的 能够交集不为空的区间的左端点 大于/等于 区间$i$的左端点\n假设给定区间$i$，左边坐标是$l$，右边是$r$，那么在$[l, r]$ 区间里面任选$k - 1$ + 选择区间$i$ 就是包含区间$i$的方案数。\n去重复的问题?\n看了一眼解答，是这样去重复的\n考虑每个区间$i$的左端点，考虑交点为$i$区间左端点$L$的情况；\n有交集的集合假设有$m$个，其中左端点等于$L$的有$x$个\n那么交集包含左端点$L$的可选个数就是$c(k,m) - c(k,m - x)$个 就是去掉所有集合都不在这些里面的哪些解\n把所有的区间交集取值为某个左端点，全部加起来就是答案了\n组合数里面有除法，此时由于数字太大没法直接算除法，需要用乘法逆元\n逆元科普：https://zhuanlan.zhihu.com/p/100587745\n","date":"2022 年 10 月 17 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1022/","section":"Posts","summary":"","title":"下午茶 1017-1022","type":"posts"},{"content":"下午茶10.10-10.14 总结: 多维dp + 特殊BFS + 经典dp\n当你想不出来的时候不妨将思维 \u0026ldquo;逆转过来\u0026rdquo;\n下午茶合集 # 10.14-1407D # 10.13-1286A # 本质：1-n排列挖掉几个数字，求一种放法，使得相邻的数，奇偶性不同的数量最小。\n考虑奇数\u0026amp;偶数可以放的数量, 以及看到相邻，立刻反应过来这是一个序列dp的问题\n状态定义:\nf[i][j][k][0/1] 表示 [0:i]位置，取了[j]个奇数，[k]个偶数，末尾取值是奇数1/偶数0的最小操作数；最终答案就是f[n][*][*][0/1]的最小值\n实际上可以优化成 f[i][j][0/1],考虑到题目中写明了\u0026rsquo;是一个$1-n$的排列\u0026rsquo;，所以一定是有$n/2$个偶数，剩下都是奇数，因此可以去掉上面的一个维度；最终答案就是f[n][n/2][0/1]的最小值\n更进一步，这里f[i][j][0/1]只和前一个状态f[i-1][**][**]有关系，所以实际上可以压缩这个维度\n#include \u0026lt;bits/stdc++.h\u0026gt; #define ll long long using namespace std; int maxn = 0x3f3f3f3f; int n; int p[105]; // f[i][j][0/1] // 前i个数, 填了j个偶数, 末尾是偶数/奇数的最小对数 int f[105][105][2]; // if: [i] = 0, could fill // f[i][j][0] \u0026lt;- f[i - 1][j - 1][0] + 0, f[i - 1][j][1] + 1, // f[i][j][1] \u0026lt;- f[i - 1][j - 1][1] + 0, f[i - 1][j][0] + 1, int main() { cin \u0026gt;\u0026gt; n; for (int i = 1; i \u0026lt;= n; ++i) { cin \u0026gt;\u0026gt; p[i]; } memset(f, 0x3f, sizeof(f)); f[0][0][0] = 0; f[0][0][1] = 0; for (int i = 1; i \u0026lt;= n; ++i) { // number of evens for (int j = 0; j \u0026lt;= n / 2; ++j) { // p[i] is even, or p[i] is 0 and filled even if (j \u0026gt; 0 \u0026amp;\u0026amp; p[i] % 2 == 0) { f[i][j][0] = min(f[i - 1][j - 1][0], f[i - 1][j - 1][1] + 1); } if (p[i] == 0 || p[i] % 2 \u0026gt; 0) { f[i][j][1] = min(f[i - 1][j][0] + 1, f[i - 1][j][1]); } } } cout \u0026lt;\u0026lt; min(f[n][n / 2][0], f[n][n / 2][1]) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; return 0; } 10.12-1651D # 输入 $n(\\lt 2e5)$ 和 $n$ 个二维平面上的互不相同的整点，坐标范围 $[1,2e5]$。 对每个整点，输出离它曼哈顿距离最近的，且不在输入中的整点。\n两点的曼哈顿距离=横坐标之差的绝对值+纵坐标之差的绝对值。\n可能有些点，它的四周就是空的，那么对于这些点而言直接返回其中一个就行了\n内侧的点怎么找到最近? 从与它相邻的外部点转移过来就是最近了\n特别的BFS技巧，从外部点(可行解)向内(待求解)的方向进行BFS，保证内部点的解是最优的。\n#include \u0026lt;bits/stdc++.h\u0026gt; #define ll long long #define PII pair\u0026lt;int,int\u0026gt; using namespace std; int n; map\u0026lt;PII, int\u0026gt; vis; int dx[4] = {1, 0, -1, 0}; int dy[4] = {0, 1, 0, -1}; PII ans[200050]; // a: new Pair, b: old Pair struct node { PII a, b; node(PII _a, PII _b): a(_a), b(_b) {} }; int main() { cin \u0026gt;\u0026gt; n; for (int i = 1; i \u0026lt;= n; ++i) { int x, y; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y; vis.insert({{x, y}, i}); } queue\u0026lt;node\u0026gt; q, qq; for (auto\u0026amp; v: vis) { auto p = v.first; // current point auto idx = v.second; // index for (int i = 0; i \u0026lt; 4; ++i) { int nx = p.first + dx[i]; // around points int ny = p.second + dy[i]; PII np = {nx, ny}; if (vis.count(np) == 0) { // not in ans[idx] = np; qq.push(node(p, np)); // [in-vis points, out-vis points] break; } } } q = qq; while (!qq.empty()) { auto pqq = qq.front(); qq.pop(); // erase old pairs vis.erase(pqq.a); } while (!q.empty()) { auto pq = q.front(); q.pop(); auto in_p = pq.a; auto out_p = pq.b; for (int i = 0; i \u0026lt; 4; ++i) { int nx = in_p.first + dx[i]; int ny = in_p.second + dy[i]; PII cur_p = {nx, ny}; if (vis.count(cur_p)) { // find! ans[vis[cur_p]] = out_p; // current point\u0026#39;s surroundings is vis.erase(cur_p); // remove visited points q.push(node(cur_p, out_p)); } } } for (int i = 1; i \u0026lt;= n; ++i) { cout \u0026lt;\u0026lt; ans[i].first \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; ans[i].second \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } return 0; } 10.11-988F # 10.10-38E # 弹珠游戏\n题目大致的含义就是求最小的一个代价, 使得所有的珠子可以滑动到对应位置。\n#include \u0026lt;bits/stdc++.h\u0026gt; #define ll long long using namespace std; struct node { int x, c; node(int _x=0, int _c=0): x(_x), c(_c) {} bool operator \u0026lt;(const node\u0026amp; a) const { return x \u0026lt; a.x; } }; int n; ll ans = LLONG_MAX; ll f[3005][3005]; node m[3050]; int main() { cin \u0026gt;\u0026gt; n; for (int i = 1; i \u0026lt;= n; ++i) { int x, c; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; c; m[i] = node(x, c); } sort(m + 1, m + n + 1); // f[i][j]: 前i个位置, 最后固定位置为j的最小代价 // 固定最左侧的 f[1][1] = m[1].c; for (int i = 2; i \u0026lt;= n; ++i) { ll oo = LLONG_MAX; for (int j = 1; j \u0026lt; i; ++j) { f[i][j] = f[i - 1][j] + (m[i].x - m[j].x); oo = min(oo, f[i - 1][j]); } f[i][i] = oo + m[i].c; } for (int ed = 1; ed \u0026lt;= n; ++ed) { ans = min(ans, f[n][ed]); } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; return 0; } ","date":"2022 年 10 月 14 日","externalUrl":null,"permalink":"/posts/2023/tea/tea-1015/","section":"Posts","summary":"","title":"下午茶 1010-1014","type":"posts"},{"content":"","externalUrl":null,"permalink":"/about/","section":"Welcome page","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]